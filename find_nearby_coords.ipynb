{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem\n",
    "\n",
    "From an interesting queston posed on stackoverflow:\n",
    "\n",
    "https://stackoverflow.com/questions/53489058/improved-efficiency-versus-iterating-over-two-large-pandas-dataframes/53490513#53490513\n",
    "\n",
    "The premise:\n",
    "With two large arrays of lat-lng coordinates, obtain the number of points a coordinate in the first array is \"close\" (within 1 km) to in the second array. So if we have two arrays (_n x 2_ and _k x 2_ where _n > k_), the resulting array should be an array of counts of dimension $n$, representing the number of times a point in the first $n$ array is within 1 km of a point within the $k$ array.\n",
    "\n",
    "However, in the case of the question posed, $n = 10,000,000$ and $k = 1,000,000$. On a naive basis, you would have to calculate $10,000,000 x 1,000,000$ or $10,000,000,000,000$ distances. Unless you can accomplish this on a nanosecond timescale, this task would take a very long time (by the question poser's own estimate, some 6000 hours).\n",
    "\n",
    "This notebook will go over several approaches:\n",
    "* A naive serial approach using `geopy`: Easy to program, probably your first pass at solving this problem\n",
    "* Parallelized Numba on CPU: Requires knowledge of `Numpy`, `Numba`, and vectorizing code, but fairly straightforward\n",
    "* Vectorized Numba-CUDA: Requires the above, plus writing `guvectorize` functions to handle complicated array-to-array calculations, and working with CUDA limitations (no `Numpy` typically)\n",
    "* Numba-CUDA Custom Kernel: Requires managing host-to-gpu memory transfers, managing the CUDA grid/thread schema, and working with the CUDA limitations\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, jit, prange, vectorize, guvectorize\n",
    "import numpy as np\n",
    "from sys import getsizeof\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1_000\n",
    "k = 100\n",
    "\n",
    "coord1 = np.zeros((n, 2), dtype=np.float32)\n",
    "coord2 = np.zeros((k, 2), dtype=np.float32)\n",
    "\n",
    "coord1[:,0] = np.random.uniform(-90, 90, n).astype(np.float32)\n",
    "coord1[:,1] = np.random.uniform(-180, 180, n).astype(np.float32)\n",
    "coord2[:,0] = np.random.uniform(-90, 90, k).astype(np.float32)\n",
    "coord2[:,1] = np.random.uniform(-180, 180, k).astype(np.float32)\n",
    "\n",
    "coord1 = np.sort(coord1,axis=0)\n",
    "coord2 = np.sort(coord2,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Python\n",
    "\n",
    "In this version, I go about solving the problem as if it was my first time solving this type of geo-spatial problem. Often, particularly in a new domain, I find that I'll lean heavily on state-of-the-art libraries, stackoverflow, and other prior work in the area.\n",
    "\n",
    "The following implementation utilizes two simple loops and the latitude filter mentioned above. Additionally, it leans on geopy for a distance function (`great_circle` which is akin to haversine distance). The `great_circle` implementation is much quicker than `geodesic`, but we'll see later that it doesn't hold up to a numba-accelerated function.\n",
    "\n",
    "Although I didn't initally, I will be using the latitude filter that was suggested and implemented in the stackoverflow post. The distance between latitudes (straight north-south) maps pretty consistently to actual distance in km (slightly varies since the Earth is not a perfect sphere). By picking the distance criterion and dividing it by 100, it should eliminate those points (based on latitude alone) that would too far away to be worth calculating the actual distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic, great_circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 µs ± 409 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit geodesic(coord1[0], coord2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.9 µs ± 85.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit great_circle(coord1[0], coord2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time comparison shows a 10x increase in speed! In theory, we should be able to process 200 million distances per hour ($3600 / 17.2 x 10^{-6}$) with this function which still falls short of the 10 trillion distances we may need to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearby_py(coord1, coord2, max_dist):\n",
    "    out = []\n",
    "    lat_filter = max_dist / 100\n",
    "    for lat,lng in coord1:\n",
    "        ct = 0\n",
    "        for lat2,lng2 in coord2:\n",
    "            if np.abs(lat - lat2) < lat_filter:\n",
    "                if great_circle((lat,lng),(lat2,lng2)).km < max_dist:\n",
    "                    ct += 1\n",
    "        out.append(ct)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 ms ± 635 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "_t_py = %timeit -o get_nearby_py(coord1, coord2, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10953.98 Est. Hours to Process the Full Data Set\n"
     ]
    }
   ],
   "source": [
    "# Estimate of time for full data set (hours)\n",
    "est_time = lambda x : print( '{} Est. Hours to Process the Full Data Set'.format( round(1e13/1e5 * x / 3600, 2) ) )\n",
    "est_time(_t_py.average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba-CPU Performance\n",
    "\n",
    "My intuition was that geopy was doing fancy things that make it a great general purpose library for geospatial analysis, but not for the brute force computations we require at the moment. My first step was to implement haversine distance to boost the distance calculation portion of solving this problem.\n",
    "\n",
    "The haversine function alone represents a near 15x improvement over `geopy.great_circle()`, allowing us to calculate 3 billion distances per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def haversine(s_lat,s_lng,e_lat,e_lng):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    s_lat = np.deg2rad(s_lat)                    \n",
    "    s_lng = np.deg2rad(s_lng)     \n",
    "    e_lat = np.deg2rad(e_lat)                       \n",
    "    e_lng = np.deg2rad(e_lng)  \n",
    "\n",
    "    d = np.sin((e_lat - s_lat)/2)**2 + np.cos(s_lat)*np.cos(e_lat) * np.sin((e_lng - s_lng)/2)**2\n",
    "\n",
    "    return 2 * R * np.arcsin(np.sqrt(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998 ns ± 5.15 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
      "3606934602\n"
     ]
    }
   ],
   "source": [
    "_t_hav = %timeit -o haversine(coord1[0,0],coord1[0,1],coord2[0,0],coord2[0,1])\n",
    "print(int(3600 / _t_hav.average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I utilized Numba to parallelize the search across the arrays. Here `prange` and the decorator `@jit(nopython=True, parallel=True)` run each loop of `for i in range(n)` as an independent process. While this will lead to a substantial speed increase, this could also lead to race conditions or cause issues if for inter-loop dependencies. Fortunately, the steps taken in this function have no such dependencies (and each loop is writing to its own space in the output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def get_nearby_numba(coord1, coord2, max_dist):\n",
    "    '''\n",
    "    Input: `coords`: List of coordinates, lat-lngs in an n x 2 array\n",
    "           `coords2`: List of port coordinates, lat-lngs in an k x 2 array\n",
    "           `max_dist`: Max distance to be considered nearby\n",
    "    Output: Array of length n with a count of coords nearby coords2\n",
    "    '''\n",
    "    # initialize\n",
    "    n = coord1.shape[0]\n",
    "    k = coord2.shape[0]\n",
    "    output = np.zeros(n, dtype=np.int32)\n",
    "    lat_filter = max_dist / 100\n",
    "    \n",
    "    # prange is an explicit parallel loop, use when operations are independent and no race conditions exist\n",
    "    for i in prange(n):\n",
    "        # comparing a point in coord1 to the arrays in coord2\n",
    "        point = coord1[i]\n",
    "        # subsetting coord2 to reduce haversine calc time.\n",
    "        coord2_filtered = coord2[np.abs(point[0] - coord2[:,0]) < lat_filter]\n",
    "        # in case of no matches\n",
    "        if coord2_filtered.shape[0] == 0: continue\n",
    "        # returns an array of length k\n",
    "        dist = haversine(point[0], point[1], coord2_filtered[:,0], coord2_filtered[:,1])\n",
    "        # sum the boolean of distances less than the max allowable\n",
    "        output[i] = np.sum(dist < max_dist)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662 µs ± 31.7 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "18.39 Est. Hours to Process the Full Data Set\n"
     ]
    }
   ],
   "source": [
    "_t_ncpu = %timeit -o get_nearby_numba(coord1, coord2, 1.0)\n",
    "est_time(_t_ncpu.average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba-CUDA guFunc Performance\n",
    "\n",
    "The `vectorize`, `guvectorize`, and `cuda.jit` decorators allow us to write functions that will be run on our GPU. However, they differ significantly between each other.\n",
    "\n",
    "`vectorize` is useful for functions that rely on straight-forward applications of broadcasting. Thinking functions of the form, in terms of input and output: (m) `+-/*` (1) -> (m)\n",
    "\n",
    "`guvectorize` is useful for more complicated arrays where broadcasting may not be obvious. For example, imagine trying to find the sum of two arrays: (m) + (n) -> (?). This may be interpreted several ways:\n",
    "- (m) + (n) -> (m), Here the resulting array may represent each elemet of m summed with every element of n.\n",
    "- (m) + (n) -> (n), Like above, except each element of n summed with elements of m.\n",
    "- (m) + (n) -> (), A reduction operation to a scalar, basically the elements of m and n summed together.\n",
    "- You can probably think of many more.\n",
    "\n",
    "With `guvectorize`, you'll often need to write loops to iterate across the dimensions of an array to arrive at your explicit intention. Additionally, the output array or scalar must be passed as a parameter and mutated (no return statement).\n",
    "\n",
    "`cuda.jit` is perhaps the most complicated, as you'll be utilizing the CUDA design pattern for programs. This will be discussed at a later section, but for now, know that `cuda.jit` allows for the creation of _kernels_ that are the most flexible way to program for a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def haversine_cuda(s_lat,s_lng,e_lat,e_lng):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    s_lat = s_lat * math.pi / 180                     \n",
    "    s_lng = s_lng * math.pi / 180 \n",
    "    e_lat = e_lat * math.pi / 180                    \n",
    "    e_lng = e_lng * math.pi / 180 \n",
    "\n",
    "    d = math.sin((e_lat - s_lat)/2)**2 + math.cos(s_lat)*math.cos(e_lat) * math.sin((e_lng - s_lng)/2)**2\n",
    "\n",
    "    return 2 * R * math.asin(math.sqrt(d))\n",
    "\n",
    "@guvectorize(['(float32[:,:], float32[:,:], float32, int32[:])'], '(m,w),(n,w),() -> (m)', target='cuda')\n",
    "def get_nearby_ufunc(coord1, coord2, max_dist, out):\n",
    "    n = coord1.shape[0]\n",
    "    k = coord2.shape[0]\n",
    "    lat_filter = max_dist / 100\n",
    "    \n",
    "    for i in range(n):\n",
    "        ct = 0\n",
    "        for j in range(k):\n",
    "            if math.fabs(coord1[i,0] - coord2[j,0]) > lat_filter: continue\n",
    "            dist = haversine_cuda(coord1[i,0], coord1[i,1], coord2[j,0], coord2[j,1])\n",
    "            if dist < max_dist: ct += 1\n",
    "        out[i] = ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1 ms ± 2.83 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "336.14 Est. Hours to Process the Full Data Set\n"
     ]
    }
   ],
   "source": [
    "out = np.empty(n, dtype=np.int32)\n",
    "_t_nguv = %timeit -o get_nearby_ufunc(coord1, coord2, 1.0, out)\n",
    "est_time(_t_nguv.average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that this seems to be a big step back in terms of performance. Part of the problem is that we're not utilizing the full potential of the hardware through a _kernel_. However, we're also not utilizing the memory on the GPU with this current pattern. Instead, we're forcing many host-to-GPU transfers that are killing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That speed up is more like it, although the Numba-CPU kernel is still leading the pack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba-CUDA Kernel Performance - Global Memory\n",
    "\n",
    "We're doing a few things differently (although the working code is mostly the same) so we'll cover the changes to memory and the kernel in two parts:\n",
    "\n",
    "#### Global Memory\n",
    "\n",
    "With just a few tweaks to how we store the arrays and variables, we can reduce the number of shuffles between host and GPU and boost performance. However, if we want to use the result of this function, we'll need to move `out_gpu` back from the device.\n",
    "\n",
    "You may also be wondering why I cast the arrays to 32-bit precision. I'm currently running this on an RTX 2070. While a powerful GPU, this model does not have strong support for [double precision (64-bit) calculations.](https://en.wikipedia.org/wiki/GeForce_20_series)\n",
    "\n",
    "#### Kernel\n",
    "\n",
    "We need to define the grid, block, thread structure of our kernel. Additionally, we have to tell the gpu who we're going to iterate over these structures while completing this work. Fortunately, Numba exposes a few helper functions like `cuda.grid()` and `cuda.gridsize()`.\n",
    "\n",
    "Basically, the first for-loop will iterate through each thread within each block, then continue on to the next block. However, we won't need to manually keep track of CUDA's numbering scheme. Instead, we'll be iterating through all of the blocks and threads but we'll get to treat i as ranging from `0, 1, 2, 3,..., End_of_the_array`. Since we know `out` will be of equal size to `coord1`, we can write directly to `out[i]` without worrying about simultaneousy read/writes. \n",
    "\n",
    "In other words, each thread will manage one element of `out`, iterate through the `coord2`, and mutate it's single element until the entire span of `coord1` is exhausted. This behaviour prevents us from having to write some kind of data-loader, which is awesome. These threads will execute in blocks of 1024, 36 grids at a time. These numbers were picked due to the number of stream processors available on my particular hardware, and trial-and-error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def get_nearby_kernel(coord1, coord2, max_dist, out):\n",
    "    start = cuda.grid(1)\n",
    "    stride = cuda.gridsize(1)\n",
    "    lat_filter = max_dist / 100\n",
    "    \n",
    "    for i in range(start, coord1.shape[0], stride):\n",
    "        out[i] = 0\n",
    "        _lat1 = coord1[i,0]\n",
    "        _lng1 = coord1[i,1]\n",
    "        \n",
    "        for j in range(coord2.shape[0]):\n",
    "            _lat2 = coord2[j,0]\n",
    "            _lng2 = coord2[j,1]\n",
    "            \n",
    "            if math.fabs(_lat1 - _lat2) > lat_filter: continue\n",
    "            \n",
    "            dist = haversine_cuda(_lat1, _lng1, _lat2, _lng2)\n",
    "            \n",
    "            if dist < max_dist: \n",
    "                out[i] += 1\n",
    "        \n",
    "threads_per_block = 1024\n",
    "blocks_per_grid = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord1_gpu = cuda.to_device(coord1)\n",
    "coord2_gpu = cuda.to_device(coord2)\n",
    "out_gpu = cuda.device_array(shape=(n,), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331 µs ± 47.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "_t_nker = %timeit -o \\\n",
    "get_nearby_kernel[blocks_per_grid, threads_per_block](coord1_gpu, coord2_gpu, 1.0, out_gpu); \\\n",
    "out_gpu.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.19 Est. Hours to Process the Full Data Set\n"
     ]
    }
   ],
   "source": [
    "est_time(_t_nker.average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result seems barely better than the Numba-CPU function but I'm betting that the overhead introduced by the GPU is clouding things here, let's see how it does with a bigger data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Great Test!\n",
    "\n",
    "Now we pit Numba-CPU vs Numba-GPU on a much larger dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Data, 100x Smaller than the Problem Posed\n",
    "n = 1_000_00\n",
    "k = 100_000\n",
    "\n",
    "coord1 = np.zeros((n, 2), dtype=np.float32)\n",
    "coord2 = np.zeros((k, 2), dtype=np.float32)\n",
    "\n",
    "coord1[:,0] = np.random.uniform(-90, 90, n).astype(np.float32)\n",
    "coord1[:,1] = np.random.uniform(-180, 180, n).astype(np.float32)\n",
    "coord2[:,0] = np.random.uniform(-90, 90, k).astype(np.float32)\n",
    "coord2[:,1] = np.random.uniform(-180, 180, k).astype(np.float32)\n",
    "\n",
    "coord1 = np.sort(coord1,axis=0)\n",
    "coord2 = np.sort(coord2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.8 s, sys: 0 ns, total: 29.8 s\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%time cpu_solution = get_nearby_numba(coord1, coord2, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA Arrays\n",
    "coord1_gpu = cuda.to_device(coord1)\n",
    "coord2_gpu = cuda.to_device(coord2)\n",
    "out_gpu = cuda.device_array(shape=(n,), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 177 ms, sys: 75.4 ms, total: 252 ms\n",
      "Wall time: 252 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "get_nearby_kernel[blocks_per_grid, threads_per_block](coord1_gpu, coord2_gpu, 1.0, out_gpu)\n",
    "gpu_solution = out_gpu.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Hooray, an RTX 2070 is about 10-15x faster at this task than a Ryzen 1600 (6-Core, 12-Threads). That's a nice little speed up for what was a first pass. The secret to getting insane speed-ups out of CUDA Kernels is to leverage **shared memory** (a high-speed cache per grid) as much as possible. However, for this problem, the two coordinate sets would exceed the limits of this cache. I'm certain that with a different grid/thread layout, I could have used shared memory, but this example wasn't too difficult to code while yielding a substantial gain in speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addendum: Arriving at a Different Solution\n",
    "\n",
    "I noticed a weird behavior that can be summarized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Match?  False\n",
      "No. of Mismatches:  14\n"
     ]
    }
   ],
   "source": [
    "print('All Match? ', np.all(cpu_solution == gpu_solution) )\n",
    "print('No. of Mismatches: ', np.sum(cpu_solution != gpu_solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1, -1, -1,  1, -1,  1,  1,  1, -1,  1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatch_mask = cpu_solution != gpu_solution\n",
    "cpu_solution[mismatch_mask] - gpu_solution[mismatch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the cpu_solution was slightly off by one from the gpu_solution. I'm willing to chalk this up to floating point imprecision. Additionally, although they're functionally the same, the two different haversine functions rely on different libraries (_cpu : numpy_ vs _gpu : math_). Given that there are 100 billion points being compared, I can live with 14 mismatches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
