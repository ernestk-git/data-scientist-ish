{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from numba import jit\n",
    "from multiprocessing import cpu_count\n",
    "from dask.multiprocessing import get\n",
    "from functools import reduce\n",
    "\n",
    "nCores = cpu_count()\n",
    "sns.set()\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Data\n",
    "\n",
    "The computer I primarily use has 32GB of RAM (DDR4-3200,CAS14).  Additionally, I created a 64GB swap file partition on a Samsung 960 Evo M.2 Flash Drive (if anyone has any experience using Intel Optane drive for this, let me know about your experiences).  I wanted to create a dataframe that exceeded 32GB in memory to test the efficacy of Pandas vs Dask vs Spark.  The following parameters should accomplish that with 100 million rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"data\" contains a random date between 1900 and 2000, a random float between 0 and 1, a random int between 0 and 3333, and a \"categorical\" string of [a-z].\n",
    "\n",
    "The random int column, c2, contains NaN values, 2% of the total.\n",
    "\n",
    "The categorical column, c3, also contains NaN values, 2% of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(nRows, startdate):\n",
    "    @jit\n",
    "    def numerical_data(nRows, startdate):\n",
    "        c1 = np.random.rand(nRows)\n",
    "        c2 = np.random.randint(0,3333,nRows)\n",
    "        c3 = np.random.choice(['a','b','c','d','e',\n",
    "                   'f','g','h','i','j',\n",
    "                   'k','l','m','n','o',\n",
    "                   'p','q','r','s','t',\n",
    "                   'u','v','w','x','y','z'],size=nRows)\n",
    "\n",
    "        return c1, c2, c3\n",
    "\n",
    "    c1, c2, c3 = numerical_data(nRows, startdate)\n",
    "    date = [startdate + datetime.timedelta(int(i)) for i in np.random.randint(1,36524,size=nRows)]\n",
    "    \n",
    "    data = pd.DataFrame({'date':date,\n",
    "                         'c1':c1,\n",
    "                         'c2':c2,\n",
    "                         'c3':c3\n",
    "                        })\n",
    "\n",
    "    data.date = pd.to_datetime(data.date)\n",
    "\n",
    "    ### picking random null values\n",
    "    data.loc[data.sample(frac=.02).index,'c2'] = np.nan\n",
    "    data.loc[data.sample(frac=.02).index,'c3'] = np.nan\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python, Pandas, Single-Processor\n",
    "\n",
    "The following function extracts the various parts of time from a datetime column and returns the original dataframe with the new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_extractor(df, dateCol, interval=['year','month','week','day','dayofweek']):\n",
    "    '''\n",
    "    input: dataframe, column of datetime objects, desired list of time-interval features\n",
    "    output: dataframe with new time-interval features appended\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for i in interval:\n",
    "        df.loc[:, i] = eval('df.%s.dt.%s'% (dateCol, i))\n",
    "    df.drop(dateCol, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function fills our missing values and creates an addition feature: `feat_isnull`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null(df, cols):\n",
    "    '''\n",
    "    input: dataframe, numerical columns with null values\n",
    "    output: dataframe with null values imputed with median and a new feature indicating that entry was null\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        df.loc[:, feat_is_null] = np.int8(df[col].isnull())\n",
    "        \n",
    "         # imputing median\n",
    "        impute_value = np.nanmedian(df[col])\n",
    "        df.loc[:, col].fillna(impute_value, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical features, we'll treat them slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null_cat(df, cols, naStr):\n",
    "    '''\n",
    "    input: dataframe, categorical column with null values, string to signify missing value\n",
    "    output: dataframe with null values imputed with 'UNK' and a new feature indicating entry was null\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        df.loc[:, feat_is_null] = np.int8(df[col].isnull())\n",
    "        \n",
    "        # imputing missing code\n",
    "        df.loc[:, col].fillna(naStr, inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions deal with encoding categorical features.  We will not be using the sklearn encoder as it has issues dealing with previously unseen values.  For example, let's say you have a feature with 6 levels: `{A, B, C, D, E, F}`, however, `F` is relatively rare.  After creating a validation set, it turns out that `F` is not seen in the `x_train` but is in `x_valid`.  \n",
    "\n",
    "Sklearn's encoder will not be able to \"train\" on `x_train` and properly convert `x_valid` due to the varying number of values.  In the functions below, instead, we pass `unkStr` to recode a previously unseen value.  For this example, we will encode these as we did missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_encode_train(df, col, unkStr):\n",
    "    '''\n",
    "    input: dataframe, name of single categorical column\n",
    "    output: dictionary representing previous levels and new numerical encodings\n",
    "    '''\n",
    "    keys = set(df[col])\n",
    "    keys.add(unkStr)\n",
    "    values = np.arange(0, len(keys)+1)\n",
    "    \n",
    "    return dict(zip(keys,values))\n",
    "\n",
    "\n",
    "def cat_encoder(df, col, dict_enc, unkStr):\n",
    "    '''\n",
    "    input: dataframe, name of single categorical column, dictionary of encodings, string to use as unknown\n",
    "    output: dataframe with categorical values encoded\n",
    "    note: you probably want to match the unknown str with the string used as null in impute_null_cat\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    # need to replace unknown values with unkStr\n",
    "    df.loc[~df[col].isin(set(dict_enc.keys())), col] = unkStr\n",
    "\n",
    "    df.loc[:,col] = df.loc[:,col].map(lambda x : dict_enc[x])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 100000\n",
      "39.5 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "3.86 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "8.48 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "1.17 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "39.3 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 1000000\n",
      "323 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "22.2 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "67.5 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "12.1 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "338 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 10000000\n",
      "4.58 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "387 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "825 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "137 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "3.51 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = ['pd_date',\n",
    "                                  'pd_impute',\n",
    "                                  'pd_encode',\n",
    "                                  'pd_total',\n",
    "                                  'nRows',\n",
    "                                  'memory'\n",
    "                                 ])\n",
    "\n",
    "# parameters for generating data\n",
    "startdate = datetime.date(1900,1,1)\n",
    "\n",
    "for i,nIter in enumerate(10 ** np.arange(5,8)): \n",
    "    data = gen_data(nIter, startdate)\n",
    "    \n",
    "    print('Size: %s'% nIter)\n",
    "    tmp_ = %timeit -n 1 -r 1 -o date_extractor(data, 'date')\n",
    "    out1_ = tmp_.average\n",
    "    \n",
    "    tmp_ = %timeit -n 1 -r 1 -o impute_null(data, ['c2'])\n",
    "    out2_ = tmp_.average\n",
    "    tmp_ = %timeit -n 1 -r 1 -o impute_null_cat(data, ['c3'], 'UNK')\n",
    "    out2_ += tmp_.average\n",
    "    \n",
    "    tmp_ = %timeit -n 1 -r 1 -o cat_encode_train(data, 'c3', 'UNK')\n",
    "    out3_ = tmp_.average\n",
    "    dict_ = cat_encode_train(data, 'c3', 'UNK')\n",
    "    tmp_ = %timeit -n 1 -r 1 -o cat_encoder(data, 'c3', dict_, 'UNK')\n",
    "    out3_ += tmp_.average\n",
    "    \n",
    "    row = {'pd_date':out1_,\n",
    "           'pd_impute':out2_,\n",
    "           'pd_encode':out3_,\n",
    "           'pd_total':out1_ + out2_ + out3_,\n",
    "           'nRows':nIter,\n",
    "           'memory':data.memory_usage().sum() / 1e6\n",
    "          }\n",
    "    \n",
    "    results.loc[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pd_date</th>\n",
       "      <th>pd_impute</th>\n",
       "      <th>pd_encode</th>\n",
       "      <th>pd_total</th>\n",
       "      <th>nRows</th>\n",
       "      <th>memory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.043549</td>\n",
       "      <td>0.01344</td>\n",
       "      <td>0.041927</td>\n",
       "      <td>0.098916</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>6.60008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pd_date  pd_impute  pd_encode  pd_total     nRows   memory\n",
       "0  0.043549    0.01344   0.041927  0.098916  100000.0  6.60008"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask, Multi-Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next section, we'll be using map_partitions to apply the previous written functions across data sets to parallelize their operations.  While we could re-write these functions, this is the quickest way to get the benefit of multi-core parallel operation.\n",
    "\n",
    "However, we're going to have to rewrite our `cat_encoder`.  Our label encoder takes all of the possible values within a feature and creates a numerical mapping.  However, since our data will be split randomly into parititons, we cannot be certain that each partition will have an identical set of values.  In this example, I split the data into 12 partitions, but for this function, I do not want 12 potential encodings.\n",
    "\n",
    "We'll change the function to, instead, aggregate all the potential encodings before creating a numerical mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_encode_train_dask(dd, col, unkStr):\n",
    "    '''\n",
    "    input: Dask Dataframe\n",
    "    output: Dictionary containing categorical-to-numerical mappings\n",
    "    note: Need to be careful to grab all potential mappings across all partitions\n",
    "    '''\n",
    "    tmp_ = dd.map_partitions(lambda df : set(df[col])).compute()\n",
    "    keys = reduce(lambda x, y : x | y, tmp_)\n",
    "    keys.add(unkStr)\n",
    "    values = np.arange(0, len(keys)+1)\n",
    "    \n",
    "    return dict(zip(keys,values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 100000\n",
      "106 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "24.2 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "43.2 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "6.16 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "278 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 1000000\n",
      "187 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "39.4 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "164 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "16 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "813 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 10000000\n",
      "1.71 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "310 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "981 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "223 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "6.74 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = ['dd_date',\n",
    "                                  'dd_impute',\n",
    "                                  'dd_encode',\n",
    "                                  'dd_total',\n",
    "                                  'nRows'\n",
    "                                 ])\n",
    "\n",
    "# parameters for generating data\n",
    "startdate = datetime.date(1900,1,1)\n",
    "\n",
    "for i,nIter in enumerate(10 ** np.arange(5,8)): \n",
    "    data_ = gen_data(nIter, startdate)\n",
    "    data = dd.from_pandas(data_,npartitions=6)\n",
    "    \n",
    "    print('Size: %s'% nIter)\n",
    "    tmp_ = %timeit -n 1 -r 1 -o  data.\\\n",
    "                                    map_partitions(lambda df : date_extractor(df,'date')).\\\n",
    "                                    compute(num_workers=12)\n",
    "    out1_ = tmp_.average\n",
    "    data = dd.from_pandas(data_,npartitions=6)   \n",
    "    \n",
    "    tmp_ = %timeit -n 1 -r 1 -o  data.\\\n",
    "                                    map_partitions(lambda df : impute_null(df, ['c2'])).\\\n",
    "                                    compute(num_workers=12)\n",
    "    out2_ = tmp_.average\n",
    "    data = dd.from_pandas(data_,npartitions=6)\n",
    "    tmp_ = %timeit -n 1 -r 1 -o  data.\\\n",
    "                                    map_partitions(lambda df : impute_null_cat(df, ['c3'], 'UNK')).\\\n",
    "                                    compute()\n",
    "    out2_ += tmp_.average\n",
    "    data = dd.from_pandas(data_,npartitions=6)\n",
    "    \n",
    "    tmp_ = %timeit -n 1 -r 1 -o cat_encode_train_dask(data, 'c3', 'UNK')\n",
    "    out3_ = tmp_.average\n",
    "    data = dd.from_pandas(data_,npartitions=6)\n",
    "    dict_ = cat_encode_train_dask(data.\\\n",
    "                                   map_partitions(lambda df : impute_null_cat(df, ['c3'], 'UNK')),\n",
    "                                  'c3',\n",
    "                                  'UNK')\n",
    "    data = dd.from_pandas(data_,npartitions=6)\n",
    "    tmp_ = %timeit -n 1 -r 1 -o data.\\\n",
    "                                    map_partitions(lambda df : date_extractor(df,'date')).\\\n",
    "                                    map_partitions(lambda df : impute_null(df, ['c2'])).\\\n",
    "                                    map_partitions(lambda df : impute_null_cat(df, ['c3'], 'UNK')).\\\n",
    "                                    map_partitions(lambda df: cat_encoder(df, 'c3', dict_, 'UNK')).\\\n",
    "                                    compute()\n",
    "    out3_ += tmp_.average\n",
    "    \n",
    "    row = {'dd_date':out1_,\n",
    "           'dd_impute':out2_,\n",
    "           'dd_encode':out3_-out2_-out1_,\n",
    "           'dd_total':out3_,\n",
    "           'nRows':nIter,\n",
    "          }\n",
    "    \n",
    "    results.loc[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dd_date</th>\n",
       "      <th>dd_impute</th>\n",
       "      <th>dd_encode</th>\n",
       "      <th>dd_total</th>\n",
       "      <th>nRows</th>\n",
       "      <th>memory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106063</td>\n",
       "      <td>0.067379</td>\n",
       "      <td>0.110574</td>\n",
       "      <td>0.284016</td>\n",
       "      <td>100000</td>\n",
       "      <td>dd.Scalar&lt;truediv..., dtype=float64&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.187082</td>\n",
       "      <td>0.202911</td>\n",
       "      <td>0.439158</td>\n",
       "      <td>0.829151</td>\n",
       "      <td>1000000</td>\n",
       "      <td>dd.Scalar&lt;truediv..., dtype=float64&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.709617</td>\n",
       "      <td>1.291369</td>\n",
       "      <td>3.959555</td>\n",
       "      <td>6.960540</td>\n",
       "      <td>10000000</td>\n",
       "      <td>dd.Scalar&lt;truediv..., dtype=float64&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dd_date  dd_impute  dd_encode  dd_total     nRows  \\\n",
       "0  0.106063   0.067379   0.110574  0.284016    100000   \n",
       "1  0.187082   0.202911   0.439158  0.829151   1000000   \n",
       "2  1.709617   1.291369   3.959555  6.960540  10000000   \n",
       "\n",
       "                                 memory  \n",
       "0  dd.Scalar<truediv..., dtype=float64>  \n",
       "1  dd.Scalar<truediv..., dtype=float64>  \n",
       "2  dd.Scalar<truediv..., dtype=float64>  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "For this section, we will need to rewrite much of the previous work to utilize Spark.  Additionally, we'll need to configure the various components of Spark when we initialize a Spark Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "conf = SparkConf().\\\n",
    "        setMaster('local[*]').\\\n",
    "        setAppName('parallel_preprocessing').\\\n",
    "        set('spark.driver.cores','1').\\\n",
    "        set('spark.num.executors','5').\\\n",
    "        set('spark.driver.memory', '4G').\\\n",
    "        set('spark.executor.memory', '5G').\\\n",
    "        set('spark.driver.maxResultSize', '5G')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqc = SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gen_data(100_000,startdate)\n",
    "\n",
    "data_schema = StructType([\n",
    "                          StructField('c1',DoubleType(),True),\n",
    "                          StructField('c2',DoubleType(),True),\n",
    "                          StructField('c3',StringType(),True),\n",
    "                          StructField('date',DateType(),True)\n",
    "                         ])\n",
    "\n",
    "data_spark = sqc.createDataFrame(data,data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+---+----------+----+-----+---+----+---------+\n",
      "|                 c1|    c2| c3|      date|year|month|day|week|dayofweek|\n",
      "+-------------------+------+---+----------+----+-----+---+----+---------+\n",
      "| 0.3366087583763079| 360.0|  g|1938-10-28|1938|   10| 28|  43|   Friday|\n",
      "| 0.3527312924524688| 459.0|  v|1901-04-11|1901|    4| 11|  15| Thursday|\n",
      "| 0.6492605202705966| 148.0|  e|1971-04-04|1971|    4|  4|  13|   Sunday|\n",
      "| 0.5057923874073894|2765.0|NaN|1961-09-14|1961|    9| 14|  37| Thursday|\n",
      "| 0.8096906716183114|1117.0|  n|1931-05-04|1931|    5|  4|  19|   Monday|\n",
      "|  0.861992255824543|2375.0|  e|1952-10-14|1952|   10| 14|  42|  Tuesday|\n",
      "| 0.5179602062487393|1551.0|  b|1950-04-15|1950|    4| 15|  15| Saturday|\n",
      "| 0.7914145094368675|2967.0|  q|1938-12-27|1938|   12| 27|  52|  Tuesday|\n",
      "| 0.3636863424960113|2996.0|  c|1908-12-09|1908|   12|  9|  50|Wednesday|\n",
      "|  0.385381987473396| 960.0|  w|1916-07-27|1916|    7| 27|  30| Thursday|\n",
      "|  0.299120805229263| 275.0|  o|1949-04-19|1949|    4| 19|  16|  Tuesday|\n",
      "| 0.9621213939026159|2524.0|  i|1980-07-26|1980|    7| 26|  30| Saturday|\n",
      "| 0.9172551918581334| 418.0|  w|1989-05-23|1989|    5| 23|  21|  Tuesday|\n",
      "|  0.682019794209569|2905.0|  q|1931-08-31|1931|    8| 31|  36|   Monday|\n",
      "| 0.4702781263541762| 578.0|  d|1972-11-25|1972|   11| 25|  47| Saturday|\n",
      "|0.16486266829308938|1255.0|  j|1968-07-17|1968|    7| 17|  29|Wednesday|\n",
      "| 0.4321519847094012|1127.0|  z|1970-04-17|1970|    4| 17|  16|   Friday|\n",
      "|0.25595762294844804|1150.0|  a|1928-06-29|1928|    6| 29|  26|   Friday|\n",
      "| 0.6630554316304896|1898.0|  h|1943-10-03|1943|   10|  3|  39|   Sunday|\n",
      "| 0.2645487939176637|1688.0|  z|1950-06-14|1950|    6| 14|  24|Wednesday|\n",
      "+-------------------+------+---+----------+----+-----+---+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_spark.\\\n",
    "withColumn('year', year('date')).\\\n",
    "withColumn('month', month('date')).\\\n",
    "withColumn('day', dayofmonth('date')).\\\n",
    "withColumn('week', weekofyear('date')).\\\n",
    "withColumn('dayofweek', date_format('date','EEEE')).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_date_extractor(df, dateCol, interval=['year','month','week','day','dayofweek']):\n",
    "    '''\n",
    "    input: dataframe, column of datetime objects, desired list of time-interval features\n",
    "    output: dataframe with new time-interval features appended\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for i in interval:\n",
    "        df.loc[:, i] = eval('df.%s.dt.%s'% (dateCol, i))\n",
    "    df.drop(dateCol, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_impute_null(sdf, cols):\n",
    "    '''\n",
    "    input: Spark Dataframe, numerical columns missing values\n",
    "    output: Spark Dataframe with missing values filled with mean\n",
    "    '''\n",
    "    for col in cols:\n",
    "        value = sdf.\\\n",
    "                filter(~isnan(col)).\\\n",
    "                select(avg(col)).\\\n",
    "                head()[0]\n",
    "        \n",
    "        sdf = sdf.na.fill({col:value})\n",
    "    \n",
    "    return sdf\n",
    "\n",
    "\n",
    "def spark_impute_null_cat(sdf, cols, unkStr):\n",
    "    '''\n",
    "    input: Spark Dataframe, categorical columns missing values, new value for missing\n",
    "    output: Spark Dataframe with missing value replaced with unkStr\n",
    "    '''\n",
    "    for col in cols:\n",
    "        sdf = sdf.withColumn(col, regexp_replace(col, 'NaN', 'UNK'))\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot((results_dates.nRows), results_dates.pandas, label='Python')\n",
    "ax.set_xlabel(\"Rows\")\n",
    "ax.set_ylabel(\"Time(Seconds)\")\n",
    "ax.set_xscale('log')\n",
    "ax.axis([1,100000,0,.05])\n",
    "ax.legend()\n",
    "#fig.savefig('./python_numba_only.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dd.from_pandas(data,npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addone(x):\n",
    "    return x + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
