{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from numba import jit\n",
    "from multiprocessing import cpu_count\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "nCores = cpu_count()\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Data\n",
    "\n",
    "The computer I primarily use has 32GB of RAM (DDR4-3200,CAS14).  Additionally, I created a 64GB swap file partition on a Samsung 960 Evo M.2 Flash Drive (if anyone has any experience using Intel Optane drive for this, let me know about your experiences).  I wanted to create a dataframe that exceeded 32GB in memory to test the efficacy of Pandas vs Dask vs Spark.  The following parameters should accomplish that with 100 million rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nRows = 10000000\n",
    "startdate = datetime.date(1900,1,1)\n",
    "geoCenter = (40.723270, -73.988371)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"data\" contains a random date between 1900 and 2000, a random float between 0 and 1, a random int between 0 and 3333, a \"categorical\" string of [a-z], and two random geo points, one slightly off the other.\n",
    "\n",
    "The random int column, c2, contains NaN values, 2% of the total.\n",
    "\n",
    "The categorical column, c3, also contains NaN values, 2% of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initializing random data\n",
    "data = pd.DataFrame({'date':[startdate + datetime.timedelta(int(i)) for i in np.random.randint(1,36524,size=nRows)],\n",
    "                     'c1':np.random.rand(nRows),\n",
    "                     'c2':np.random.randint(0,3333,nRows),\n",
    "                     'c3':[random.choice(string.ascii_letters).lower() for i in range(nRows)],\n",
    "                     'lat_1':np.random.rand(nRows) + geoCenter[0],\n",
    "                     'lon_1':np.random.rand(nRows) + geoCenter[1]\n",
    "                    })\n",
    "\n",
    "data['lat_2'] = data['lat_1']\n",
    "data['lon_2'] = data['lon_1'] + 0.003\n",
    "\n",
    "data.date = pd.to_datetime(data.date)\n",
    "\n",
    "### picking random null values\n",
    "data.loc[data.sample(frac=.02).index,'c2'] = np.nan\n",
    "data.loc[data.sample(frac=.02).index,'c3'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_feather('/tmp/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather('/tmp/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing Steps\n",
    "\n",
    "### Dates\n",
    "\n",
    "In column `date` we have datetime objects as if they had been parsed by pandas `read_csv` with `parse_dates`.  These are YYYY-MM-DD style, with no additional time information.\n",
    "\n",
    "In most machine learning algorithims, we would want to separate out the individual components of a date into their own feature.  For this date, we will create five features: the year, month, week, day, and day of the week.\n",
    "\n",
    "### Filling Missing Values\n",
    "\n",
    "For missing values in column `c2` we're going to impute the mean value.  Additionally, we're going to create a new feature `c2_isnull` so that our models will be able to, hopefully, pick up on the fact that value is artificial.\n",
    "\n",
    "We'll deal with the missing values in column `c3` slightly differently, using a keyword to signify an unknown quantity.\n",
    "\n",
    "### Encoding Categorical Variables\n",
    "\n",
    "Scikit-learn comes with a `LabelEncoder` but we'll be using one from scratch.  Unfortunately, scikit-learn's encoder does not play well with new/missing values.  \n",
    "\n",
    "We know that column `c3` contains letters `a-z`, but let's say that in our train-test split, our training set was missing the letter `z`.  `LabelEncoder` throws an error when attempting to encode `z` in our test set.  Instead, we would rather a separate value be encoded, like `-1`, to signify that the true encoding is unknown.  Likewise, since this column contains missing values, we would also impute `-1` for nulls in our training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python, Pandas, Single-Processor\n",
    "\n",
    "The following function extracts the various parts of time from a datetime column and returns the original dataframe with the new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input: dataframe, column of datetime objects, desired list of time-interval features\n",
    "### output: dataframe with new time-interval features appended\n",
    "def date_extractor(df, dateCol, interval=['year','month','week','day','dayofweek']):\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for i in interval:\n",
    "        df.loc[:, i] = eval('df.%s.dt.%s'% (dateCol, i))\n",
    "    df.drop(dateCol, axis=1, inplace=True)\n",
    "    \n",
    "    return 'Added %s as Features' % ' '.join(interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function fills our missing values and creates an addition feature: `feat_isnull`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input: dataframe, numerical columns with null values\n",
    "### output: dataframe with null values imputed with median and a new feature indicating that entry was null\n",
    "def impute_null(df, cols):\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        df.loc[:, feat_is_null] = np.int8(df[col].isnull())\n",
    "        \n",
    "         # imputing median\n",
    "        impute_value = np.nanmedian(df[col])\n",
    "        df.loc[:, col].fillna(impute_value, inplace=True)\n",
    "    \n",
    "    return 'Nulls Imputed on %s' % ' '.join(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical features, we'll treat them slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input: dataframe, categorical column with null values, string to signify missing value\n",
    "### output: dataframe with null values imputed with 'UNK' and a new feature indicating entry was null\n",
    "def impute_null_cat(df, cols, naStr):\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        df.loc[:, feat_is_null] = np.int8(df[col].isnull())\n",
    "        \n",
    "        # imputing missing code\n",
    "        df.loc[:, col].fillna(naStr, inplace=True)\n",
    "        \n",
    "    return 'Nulls Imputed on %s' % ' '.join(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions deal with encoding categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input: dataframe, name of single categorical column\n",
    "### output: dictionary representing previous levels and new numerical encodings\n",
    "def cat_encode_train(df, col):\n",
    "    keys = set(df[col])\n",
    "    values = np.arange(0, len(keys))\n",
    "    \n",
    "    return dict(zip(keys,values))\n",
    "\n",
    "### input: dataframe, name of single categorical column, dictionary of encodings, string to use as unknown\n",
    "### output: dataframe with categorical values encoded\n",
    "### note: you probably want to match the unknown str with the string used as null in impute_null_cat\n",
    "def cat_encoder(df, col, dict_enc, unkStr):\n",
    "    # need to replace unknown values with unkStr\n",
    "    df.loc[~data[col].isin(set(dict_enc.keys())), col] = unkStr\n",
    "\n",
    "    df.loc[:,col].replace(dict_enc, inplace=True)\n",
    "    \n",
    "    return '%s Encoded' % col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 100000\n",
      "73.2 ms ± 7.31 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "The slowest run took 7.54 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "470 ms ± 452 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "987 ms ± 470 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "326 ms ± 78.7 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "The slowest run took 7.95 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "5.92 s ± 5.79 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "Size: 1000000\n",
      "745 ms ± 53.2 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "153 ms ± 12.4 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "14.3 ms ± 485 µs per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "189 ms ± 3.33 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "1.7 s ± 16.4 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "Size: 10000000\n",
      "10.3 s ± 504 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "146 ms ± 2.58 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "17.9 ms ± 645 µs per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "251 ms ± 44.2 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "1.86 s ± 72.5 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = ['pd_date',\n",
    "                                  'pd_impute',\n",
    "                                  'pd_encode',\n",
    "                                  'pd_total',\n",
    "                                  'nRows'\n",
    "                                 ])\n",
    "\n",
    "for i,nIter in enumerate(10 ** np.arange(5,8)): \n",
    "    print('Size: %s'% nIter)\n",
    "    tmp_ = %timeit -n 1 -r 3 -o date_extractor(data.head(nIter), 'date')\n",
    "    out1_ = tmp_.average\n",
    "    \n",
    "    tmp_ = %timeit -n 1 -r 3 -o impute_null(data, ['c2'])\n",
    "    out2_ = tmp_.average\n",
    "    tmp_ = %timeit -n 1 -r 3 -o impute_null_cat(data, ['c3'], 'UNK')\n",
    "    out2_ += tmp_.average\n",
    "    \n",
    "    tmp_ = %timeit -n 1 -r 3 -o dict_ = cat_encode_train(data, 'c3')\n",
    "    out3_ = tmp_.average\n",
    "    tmp_ = %timeit -n 1 -r 3 -o cat_encoder(data, 'c3', cat_encode_train(data,'c3'), 'UNK')\n",
    "    out3_ += tmp_.average\n",
    "    \n",
    "    row = {'pd_date':out1_,\n",
    "           'pd_impute':out2_,\n",
    "           'pd_encode':out3_,\n",
    "           'pd_total':out1_ + out2_ + out3_,\n",
    "           'nRows':nIter\n",
    "          }\n",
    "    \n",
    "    results.loc[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot((results_dates.nRows), results_dates.pandas, label='Python')\n",
    "ax.set_xlabel(\"Rows\")\n",
    "ax.set_ylabel(\"Time(Seconds)\")\n",
    "ax.set_xscale('log')\n",
    "ax.axis([1,100000,0,.05])\n",
    "ax.legend()\n",
    "#fig.savefig('./python_numba_only.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setMaster('local[*]').setAppName('parallel_preprocessing')\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
