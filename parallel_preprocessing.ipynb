{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from numba import jit\n",
    "from multiprocessing import cpu_count\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "nCores = cpu_count()\n",
    "sns.set()\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Data\n",
    "\n",
    "The computer I primarily use has 32GB of RAM (DDR4-3200,CAS14).  Additionally, I created a 64GB swap file partition on a Samsung 960 Evo M.2 Flash Drive (if anyone has any experience using Intel Optane drive for this, let me know about your experiences).  I wanted to create a dataframe that exceeded 32GB in memory to test the efficacy of Pandas vs Dask vs Spark.  The following parameters should accomplish that with 100 million rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"data\" contains a random date between 1900 and 2000, a random float between 0 and 1, a random int between 0 and 3333, a \"categorical\" string of [a-z], and two random geo points, one slightly off the other.\n",
    "\n",
    "The random int column, c2, contains NaN values, 2% of the total.\n",
    "\n",
    "The categorical column, c3, also contains NaN values, 2% of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.04105638, 0.62226792, 0.07476097, 0.98413162, 0.70467256,\n",
       "        0.37125388, 0.20766254, 0.15509163, 0.31475989, 0.97976033]),\n",
       " array([  80, 3087, 1059, 2892, 2739, 2069, 2028,  104,  278,  736]),\n",
       " array(['i', 'z', 'j', 's', 'a', 'u', 'u', 'k', 'u', 'l'], dtype='<U1'),\n",
       " array([41.29579226, 41.00267357, 41.68695102, 41.06827429, 41.30578892,\n",
       "        40.82994656, 41.41195584, 41.72318209, 41.0725217 , 41.33174169]),\n",
       " array([-73.42558559, -73.06823442, -73.63846969, -73.75346369,\n",
       "        -73.8743908 , -73.25780152, -73.75313524, -73.19495687,\n",
       "        -73.42089353, -73.95370145]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def gen_data(nRows, startdate, geoCenter):\n",
    "    #date = [startdate + datetime.timedelta(int(i)) for i in np.random.randint(1,36524,size=nRows)]\n",
    "    c1 = np.random.rand(nRows)\n",
    "    c2 = np.random.randint(0,3333,nRows)\n",
    "    c3 = np.random.choice(['a','b','c','d','e',\n",
    "               'f','g','h','i','j',\n",
    "               'k','l','m','n','o',\n",
    "               'p','q','r','s','t',\n",
    "               'u','v','w','x','y','z'],size=nRows)\n",
    "    lat_1 = np.random.rand(nRows) + geoCenter[0]\n",
    "    lon_1 = np.random.rand(nRows) + geoCenter[1]\n",
    "                        \n",
    "    return c1, c2, c3, lat_1, lon_1\n",
    "\n",
    "gen_data(nRows,startdate,geoCenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "nRows = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 540 ms, total: 16.8 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "### initializing random data\n",
    "%time data = pd.DataFrame({'date':[startdate + datetime.timedelta(int(i)) for i in np.random.randint(1,36524,size=nRows)],'c1':np.random.rand(nRows),'c2':np.random.randint(0,3333,nRows),'c3':[random.choice(string.ascii_letters).lower() for i in range(nRows)],'lat_1':np.random.rand(nRows) + geoCenter[0],'lon_1':np.random.rand(nRows) + geoCenter[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 980 ms, sys: 156 ms, total: 1.14 s\n",
      "Wall time: 1.14 s\n",
      "CPU times: user 6.23 s, sys: 159 ms, total: 6.39 s\n",
      "Wall time: 6.39 s\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def gen_data(nRows, startdate, geoCenter):\n",
    "    #date = [startdate + datetime.timedelta(int(i)) for i in np.random.randint(1,36524,size=nRows)]\n",
    "    c1 = np.random.rand(nRows)\n",
    "    c2 = np.random.randint(0,3333,nRows)\n",
    "    c3 = np.random.choice(['a','b','c','d','e',\n",
    "               'f','g','h','i','j',\n",
    "               'k','l','m','n','o',\n",
    "               'p','q','r','s','t',\n",
    "               'u','v','w','x','y','z'],size=nRows)\n",
    "    lat_1 = np.random.rand(nRows) + geoCenter[0]\n",
    "    lon_1 = np.random.rand(nRows) + geoCenter[1]\n",
    "                        \n",
    "    return c1, c2, c3, lat_1, lon_1\n",
    "    \n",
    "### initializing random data\n",
    "#data = pd.DataFrame({'date':[startdate + datetime.timedelta(int(i)) for i in np.random.randint(1,36524,size=nRows)],\n",
    "#                     'c1':np.random.rand(nRows),\n",
    "#                     'c2':np.random.randint(0,3333,nRows),\n",
    "#                     'c3':[random.choice(string.ascii_letters).lower() for i in range(nRows)],\n",
    "#                     'lat_1':np.random.rand(nRows) + geoCenter[0],\n",
    "#                     'lon_1':np.random.rand(nRows) + geoCenter[1]\n",
    "#                    })\n",
    "%time c1, c2, c3, lat_1, lon_1 = gen_data(nRows, startdate, geoCenter)\n",
    "%time date = [startdate + datetime.timedelta(int(i)) for i in np.random.randint(1,36524,size=nRows)]\n",
    "data = pd.DataFrame({'date':date,\n",
    "                     'c1':c1,\n",
    "                     'c2':c2,\n",
    "                     'c3':c3,\n",
    "                     'lat_1':lat_1,\n",
    "                     'lon_1':lon_1\n",
    "                    })\n",
    "\n",
    "data['lat_2'] = data['lat_1']\n",
    "data['lon_2'] = data['lon_1'] + 0.003\n",
    "\n",
    "data.date = pd.to_datetime(data.date)\n",
    "\n",
    "### picking random null values\n",
    "data.loc[data.sample(frac=.02).index,'c2'] = np.nan\n",
    "data.loc[data.sample(frac=.02).index,'c3'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>date</th>\n",
       "      <th>lat_1</th>\n",
       "      <th>lon_1</th>\n",
       "      <th>lat_2</th>\n",
       "      <th>lon_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.475479</td>\n",
       "      <td>1.0</td>\n",
       "      <td>s</td>\n",
       "      <td>1938-09-17</td>\n",
       "      <td>41.581084</td>\n",
       "      <td>-73.593358</td>\n",
       "      <td>41.581084</td>\n",
       "      <td>-73.590358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.362571</td>\n",
       "      <td>2061.0</td>\n",
       "      <td>u</td>\n",
       "      <td>1945-04-20</td>\n",
       "      <td>41.695716</td>\n",
       "      <td>-73.841167</td>\n",
       "      <td>41.695716</td>\n",
       "      <td>-73.838167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.815786</td>\n",
       "      <td>836.0</td>\n",
       "      <td>a</td>\n",
       "      <td>1931-02-03</td>\n",
       "      <td>41.321212</td>\n",
       "      <td>-73.943495</td>\n",
       "      <td>41.321212</td>\n",
       "      <td>-73.940495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.774659</td>\n",
       "      <td>2575.0</td>\n",
       "      <td>a</td>\n",
       "      <td>1999-12-13</td>\n",
       "      <td>41.321807</td>\n",
       "      <td>-73.906446</td>\n",
       "      <td>41.321807</td>\n",
       "      <td>-73.903446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.226543</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>g</td>\n",
       "      <td>1973-12-20</td>\n",
       "      <td>41.554218</td>\n",
       "      <td>-73.649110</td>\n",
       "      <td>41.554218</td>\n",
       "      <td>-73.646110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.216186</td>\n",
       "      <td>565.0</td>\n",
       "      <td>a</td>\n",
       "      <td>1951-09-02</td>\n",
       "      <td>41.412864</td>\n",
       "      <td>-73.465834</td>\n",
       "      <td>41.412864</td>\n",
       "      <td>-73.462834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.261915</td>\n",
       "      <td>3280.0</td>\n",
       "      <td>t</td>\n",
       "      <td>1976-12-22</td>\n",
       "      <td>40.835462</td>\n",
       "      <td>-73.335047</td>\n",
       "      <td>40.835462</td>\n",
       "      <td>-73.332047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.438692</td>\n",
       "      <td>2589.0</td>\n",
       "      <td>l</td>\n",
       "      <td>1944-04-28</td>\n",
       "      <td>40.744131</td>\n",
       "      <td>-73.235372</td>\n",
       "      <td>40.744131</td>\n",
       "      <td>-73.232372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.900466</td>\n",
       "      <td>692.0</td>\n",
       "      <td>i</td>\n",
       "      <td>1918-08-17</td>\n",
       "      <td>41.307570</td>\n",
       "      <td>-73.380536</td>\n",
       "      <td>41.307570</td>\n",
       "      <td>-73.377536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.911113</td>\n",
       "      <td>1601.0</td>\n",
       "      <td>l</td>\n",
       "      <td>1983-11-01</td>\n",
       "      <td>41.390585</td>\n",
       "      <td>-73.776743</td>\n",
       "      <td>41.390585</td>\n",
       "      <td>-73.773743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         c1      c2 c3       date      lat_1      lon_1      lat_2      lon_2\n",
       "0  0.475479     1.0  s 1938-09-17  41.581084 -73.593358  41.581084 -73.590358\n",
       "1  0.362571  2061.0  u 1945-04-20  41.695716 -73.841167  41.695716 -73.838167\n",
       "2  0.815786   836.0  a 1931-02-03  41.321212 -73.943495  41.321212 -73.940495\n",
       "3  0.774659  2575.0  a 1999-12-13  41.321807 -73.906446  41.321807 -73.903446\n",
       "4  0.226543  1266.0  g 1973-12-20  41.554218 -73.649110  41.554218 -73.646110\n",
       "5  0.216186   565.0  a 1951-09-02  41.412864 -73.465834  41.412864 -73.462834\n",
       "6  0.261915  3280.0  t 1976-12-22  40.835462 -73.335047  40.835462 -73.332047\n",
       "7  0.438692  2589.0  l 1944-04-28  40.744131 -73.235372  40.744131 -73.232372\n",
       "8  0.900466   692.0  i 1918-08-17  41.307570 -73.380536  41.307570 -73.377536\n",
       "9  0.911113  1601.0  l 1983-11-01  41.390585 -73.776743  41.390585 -73.773743"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_feather('/tmp/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather('/tmp/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing Steps\n",
    "\n",
    "### Dates\n",
    "\n",
    "In column `date` we have datetime objects as if they had been parsed by pandas `read_csv` with `parse_dates`.  These are YYYY-MM-DD style, with no additional time information.\n",
    "\n",
    "In most machine learning algorithims, we would want to separate out the individual components of a date into their own feature.  For this date, we will create five features: the year, month, week, day, and day of the week.\n",
    "\n",
    "### Filling Missing Values\n",
    "\n",
    "For missing values in column `c2` we're going to impute the mean value.  Additionally, we're going to create a new feature `c2_isnull` so that our models will be able to, hopefully, pick up on the fact that value is artificial.\n",
    "\n",
    "We'll deal with the missing values in column `c3` slightly differently, using a keyword to signify an unknown quantity.\n",
    "\n",
    "### Encoding Categorical Variables\n",
    "\n",
    "Scikit-learn comes with a `LabelEncoder` but we'll be using one from scratch.  Unfortunately, scikit-learn's encoder does not play well with new/missing values.  \n",
    "\n",
    "We know that column `c3` contains letters `a-z`, but let's say that in our train-test split, our training set was missing the letter `z`.  `LabelEncoder` throws an error when attempting to encode `z` in our test set.  Instead, we would rather a separate value be encoded, like `-1`, to signify that the true encoding is unknown.  Likewise, since this column contains missing values, we would also impute `-1` for nulls in our training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python, Pandas, Single-Processor\n",
    "\n",
    "The following function extracts the various parts of time from a datetime column and returns the original dataframe with the new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_extractor(df, dateCol, interval=['year','month','week','day','dayofweek']):\n",
    "    '''\n",
    "    input: dataframe, column of datetime objects, desired list of time-interval features\n",
    "    output: dataframe with new time-interval features appended\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for i in interval:\n",
    "        df.loc[:, i] = eval('df.%s.dt.%s'% (dateCol, i))\n",
    "    df.drop(dateCol, axis=1, inplace=True)\n",
    "    \n",
    "    return 'Added %s as Features' % ' '.join(interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function fills our missing values and creates an addition feature: `feat_isnull`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null(df, cols):\n",
    "    '''\n",
    "    input: dataframe, numerical columns with null values\n",
    "    output: dataframe with null values imputed with median and a new feature indicating that entry was null\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        df.loc[:, feat_is_null] = np.int8(df[col].isnull())\n",
    "        \n",
    "         # imputing median\n",
    "        impute_value = np.nanmedian(df[col])\n",
    "        df.loc[:, col].fillna(impute_value, inplace=True)\n",
    "    \n",
    "    return 'Nulls Imputed on %s' % ' '.join(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical features, we'll treat them slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null_cat(df, cols, naStr):\n",
    "    '''\n",
    "    input: dataframe, categorical column with null values, string to signify missing value\n",
    "    output: dataframe with null values imputed with 'UNK' and a new feature indicating entry was null\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        df.loc[:, feat_is_null] = np.int8(df[col].isnull())\n",
    "        \n",
    "        # imputing missing code\n",
    "        df.loc[:, col].fillna(naStr, inplace=True)\n",
    "        \n",
    "    return 'Nulls Imputed on %s' % ' '.join(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions deal with encoding categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_encode_train(df, col):\n",
    "    '''\n",
    "    input: dataframe, name of single categorical column\n",
    "    output: dictionary representing previous levels and new numerical encodings\n",
    "    '''\n",
    "    keys = set(df[col])\n",
    "    values = np.arange(0, len(keys))\n",
    "    \n",
    "    return dict(zip(keys,values))\n",
    "\n",
    "\n",
    "def cat_encoder(df, col, dict_enc, unkStr):\n",
    "    '''\n",
    "    input: dataframe, name of single categorical column, dictionary of encodings, string to use as unknown\n",
    "    output: dataframe with categorical values encoded\n",
    "    note: you probably want to match the unknown str with the string used as null in impute_null_cat\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    # need to replace unknown values with unkStr\n",
    "    df.loc[~data[col].isin(set(dict_enc.keys())), col] = unkStr\n",
    "\n",
    "    df.loc[:,col] = df.loc[:,col].map(lambda x : dict_enc[x])\n",
    "    \n",
    "    return '%s Encoded' % col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 100000\n",
      "52.3 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "5.69 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "22.1 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "9.98 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "61.8 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 1000000\n",
      "555 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "64.3 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "143 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "53.2 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "427 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 10000000\n",
      "6.17 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "643 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "1.34 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "477 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "4.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns = ['pd_date',\n",
    "                                  'pd_impute',\n",
    "                                  'pd_encode',\n",
    "                                  'pd_total',\n",
    "                                  'nRows'\n",
    "                                 ])\n",
    "\n",
    "for i,nIter in enumerate(10 ** np.arange(5,8)): \n",
    "    ### Parameters for generating data\n",
    "    startdate = datetime.date(1900,1,1)\n",
    "    geoCenter = (40.723270, -73.988371)\n",
    "    data = gen_data(nIter, startdate, geoCenter)\n",
    "    \n",
    "    print('Size: %s'% nIter)\n",
    "    tmp_ = %timeit -n 1 -r 1 -o date_extractor(data, 'date')\n",
    "    out1_ = tmp_.average\n",
    "    \n",
    "    tmp_ = %timeit -n 1 -r 1 -o impute_null(data, ['c2'])\n",
    "    out2_ = tmp_.average\n",
    "    tmp_ = %timeit -n 1 -r 1 -o impute_null_cat(data, ['c3'], 'UNK')\n",
    "    out2_ += tmp_.average\n",
    "    \n",
    "    tmp_ = %timeit -n 1 -r 1 -o dict_ = cat_encode_train(data, 'c3')\n",
    "    out3_ = tmp_.average\n",
    "    tmp_ = %timeit -n 1 -r 1 -o cat_encoder(data, 'c3', cat_encode_train(data,'c3'), 'UNK')\n",
    "    out3_ += tmp_.average\n",
    "    \n",
    "    row = {'pd_date':out1_,\n",
    "           'pd_impute':out2_,\n",
    "           'pd_encode':out3_,\n",
    "           'pd_total':out1_ + out2_ + out3_,\n",
    "           'nRows':nIter\n",
    "          }\n",
    "    \n",
    "    results.loc[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pd_date</th>\n",
       "      <th>pd_impute</th>\n",
       "      <th>pd_encode</th>\n",
       "      <th>pd_total</th>\n",
       "      <th>nRows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052303</td>\n",
       "      <td>0.027770</td>\n",
       "      <td>0.071768</td>\n",
       "      <td>0.151841</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555055</td>\n",
       "      <td>0.206990</td>\n",
       "      <td>0.480368</td>\n",
       "      <td>1.242412</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.166593</td>\n",
       "      <td>1.986323</td>\n",
       "      <td>4.575245</td>\n",
       "      <td>12.728162</td>\n",
       "      <td>10000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pd_date  pd_impute  pd_encode   pd_total       nRows\n",
       "0  0.052303   0.027770   0.071768   0.151841    100000.0\n",
       "1  0.555055   0.206990   0.480368   1.242412   1000000.0\n",
       "2  6.166593   1.986323   4.575245  12.728162  10000000.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%lprun -f cat_encoder cat_encoder(test_,'c3',dict_,'UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.13 ms, sys: 0 ns, total: 1.13 ms\n",
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%time dict_ = cat_encode_train(data.head(10000), 'c3')\n",
    "#%time cat_encoder(data.head(10000), 'c3', dict_, 'UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('./results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot((results_dates.nRows), results_dates.pandas, label='Python')\n",
    "ax.set_xlabel(\"Rows\")\n",
    "ax.set_ylabel(\"Time(Seconds)\")\n",
    "ax.set_xscale('log')\n",
    "ax.axis([1,100000,0,.05])\n",
    "ax.legend()\n",
    "#fig.savefig('./python_numba_only.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setMaster('local[*]').setAppName('parallel_preprocessing')\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
