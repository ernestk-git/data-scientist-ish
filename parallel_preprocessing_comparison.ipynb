{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from numba import jit\n",
    "from multiprocessing import cpu_count\n",
    "from dask.multiprocessing import get\n",
    "from functools import reduce\n",
    "\n",
    "nCores = cpu_count()\n",
    "sns.set()\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_range = 10 ** np.arange(5,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Data\n",
    "\n",
    "The computer I primarily use has 32GB of RAM (DDR4-3200,CAS14).  Additionally, I created a 64GB swap file partition on a Samsung 960 Evo M.2 Flash Drive (if anyone has any experience using Intel Optane drive for this, let me know about your experiences).  I wanted to create a dataframe that exceeded 32GB in memory to test the efficacy of Pandas vs Dask vs Spark.  The following parameters should accomplish that with 100 million rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"data\" contains a random date between 1900 and 2000, a random float between 0 and 1, a random int between 0 and 3333, and a \"categorical\" string of [a-z].\n",
    "\n",
    "The random int column, c2, contains NaN values, 10% of the total.\n",
    "\n",
    "The categorical column, c3, also contains NaN values, 10% of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(nRows, startdate):\n",
    "    @jit\n",
    "    def numerical_data(nRows, startdate):\n",
    "        c1 = np.random.rand(nRows)\n",
    "        c2 = np.random.randint(0,3333,nRows)\n",
    "        c3 = np.random.choice(['a','b','c','d','e',\n",
    "                   'f','g','h','i','j',\n",
    "                   'k','l','m','n','o',\n",
    "                   'p','q','r','s','t',\n",
    "                   'u','v','w','x','y','z'],size=nRows)\n",
    "\n",
    "        return c1, c2, c3\n",
    "\n",
    "    c1, c2, c3 = numerical_data(nRows, startdate)\n",
    "    date = [startdate + datetime.timedelta(int(i)) for i in np.random.randint(1,36524,size=nRows)]\n",
    "    \n",
    "    data = pd.DataFrame({'date':date,\n",
    "                         'c1':c1,\n",
    "                         'c2':c2,\n",
    "                         'c3':c3\n",
    "                        })\n",
    "\n",
    "    data.date = pd.to_datetime(data.date)\n",
    "\n",
    "    ### picking random null values\n",
    "    data.loc[data.sample(frac=.1).index,'c2'] = np.nan\n",
    "    data.loc[data.sample(frac=.1).index,'c3'] = np.nan\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python, Pandas, Single-Processor\n",
    "\n",
    "The following function extracts the various parts of time from a datetime column and returns the original dataframe with the new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_extractor(df, dateCol, interval=['year','month','week','day','dayofweek']):\n",
    "    '''\n",
    "    input: dataframe, column of datetime objects, desired list of time-interval features\n",
    "    output: dataframe with new time-interval features appended\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for i in interval:\n",
    "        df.loc[:, i] = eval('df.%s.dt.%s'% (dateCol, i))\n",
    "    df.drop(dateCol, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function fills our missing values and creates an addition feature: `feat_isnull`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null(df, cols):\n",
    "    '''\n",
    "    input: dataframe, numerical columns with null values\n",
    "    output: dataframe with null values imputed with mean and a new feature indicating that entry was null\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        df.loc[:, feat_is_null] = np.int8(df[col].isnull())\n",
    "        \n",
    "         # imputing median\n",
    "        impute_value = np.nanmean(df[col])\n",
    "        df.loc[:, col].fillna(impute_value, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical features, we'll treat them slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null_cat(df, cols, naStr):\n",
    "    '''\n",
    "    input: dataframe, categorical column with null values, string to signify missing value\n",
    "    output: dataframe with null values imputed with 'UNK' and a new feature indicating entry was null\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        df.loc[:, feat_is_null] = np.int8(df[col].isnull())\n",
    "        \n",
    "        # imputing missing code\n",
    "        df.loc[:, col].fillna(naStr, inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions deal with encoding categorical features.  We will not be using the sklearn encoder as it has issues dealing with previously unseen values.  For example, let's say you have a feature with 6 levels: `{A, B, C, D, E, F}`, however, `F` is relatively rare.  After creating a validation set, it turns out that `F` is not seen in the `x_train` but is in `x_valid`.  \n",
    "\n",
    "Sklearn's encoder may not be able to \"train\" on `x_train` and properly convert `x_valid` due to the varying number of values.  In the functions below, instead, we pass `unkStr` to recode a previously unseen value.  For this example, we will encode these as we did missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_encode_train(df, col, unkStr):\n",
    "    '''\n",
    "    input: dataframe, name of single categorical column\n",
    "    output: dictionary representing previous levels and new numerical encodings\n",
    "    '''\n",
    "    keys = set(df[col])\n",
    "    keys.add(unkStr)\n",
    "    values = np.arange(0, len(keys)+1)\n",
    "    \n",
    "    return dict(zip(keys,values))\n",
    "\n",
    "\n",
    "def cat_encoder(df, col, dict_enc, unkStr):\n",
    "    '''\n",
    "    input: dataframe, name of single categorical column, dictionary of encodings, string to use as unknown\n",
    "    output: dataframe with categorical values encoded\n",
    "    note: you probably want to match the unknown str with the string used as null in impute_null_cat\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    # need to replace unknown values with unkStr\n",
    "    df.loc[~df[col].isin(set(dict_enc.keys())), col] = unkStr\n",
    "\n",
    "    df.loc[:,col] = df.loc[:,col].map(lambda x : dict_enc[x])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 100000\n",
      "45.3 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "4.48 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "8.44 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "1.2 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "35.4 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 1000000\n",
      "381 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "24.5 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "75.6 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "12.9 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "344 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "results_py = pd.DataFrame(columns = [\n",
    "                                     'pd_date',\n",
    "                                     'pd_impute',\n",
    "                                     'pd_encode',\n",
    "                                     'pd_total',\n",
    "                                     'nRows',\n",
    "                                     'memory'\n",
    "                                    ])\n",
    "\n",
    "# parameters for generating data\n",
    "startdate = datetime.date(1900,1,1)\n",
    "\n",
    "for i,nIter in enumerate(row_range): \n",
    "    data = gen_data(nIter, startdate)\n",
    "    \n",
    "    # date features\n",
    "    print('Size: %s'% nIter)\n",
    "    tmp_ = %timeit -n 1 -r 1 -o date_extractor(data, 'date')\n",
    "    out1_ = tmp_.average\n",
    "    \n",
    "    # imputing nulls\n",
    "    tmp_ = %timeit -n 1 -r 1 -o impute_null(data, ['c2'])\n",
    "    out2_ = tmp_.average\n",
    "    tmp_ = %timeit -n 1 -r 1 -o impute_null_cat(data, ['c3'], 'UNK')\n",
    "    out2_ += tmp_.average\n",
    "    \n",
    "    # encode cat features\n",
    "    tmp_ = %timeit -n 1 -r 1 -o cat_encode_train(data, 'c3', 'UNK')\n",
    "    out3_ = tmp_.average\n",
    "    dict_ = cat_encode_train(data, 'c3', 'UNK')\n",
    "    tmp_ = %timeit -n 1 -r 1 -o cat_encoder(data, 'c3', dict_, 'UNK')\n",
    "    out3_ += tmp_.average\n",
    "    \n",
    "    row = {'pd_date':out1_,\n",
    "           'pd_impute':out2_,\n",
    "           'pd_encode':out3_,\n",
    "           'pd_total':out1_ + out2_ + out3_,\n",
    "           'nRows':nIter,\n",
    "           'memory':data.memory_usage().sum() / 1e6\n",
    "          }\n",
    "    \n",
    "    results_py.loc[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pd_date</th>\n",
       "      <th>pd_impute</th>\n",
       "      <th>pd_encode</th>\n",
       "      <th>pd_total</th>\n",
       "      <th>nRows</th>\n",
       "      <th>memory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.045273</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.036581</td>\n",
       "      <td>0.094779</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>6.60008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.381167</td>\n",
       "      <td>0.100056</td>\n",
       "      <td>0.356497</td>\n",
       "      <td>0.837720</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>66.00008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pd_date  pd_impute  pd_encode  pd_total      nRows    memory\n",
       "0  0.045273   0.012925   0.036581  0.094779   100000.0   6.60008\n",
       "1  0.381167   0.100056   0.356497  0.837720  1000000.0  66.00008"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask, Multi-Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next section, we'll be using map_partitions to apply the previous written functions across data sets to parallelize their operations.  While we could re-write these functions, this is the quickest way to get the benefit of multi-core parallel operation.  However, we're going to have to rewrite our `impute_null()` and `cat_encoder()` functions.  \n",
    "\n",
    "For imputing the mean on a continuous variable, we'll need to collect all possible values and calculate the mean (instead of taking the mean across partitions, and finding the mean of that).  This next block illustrate that these are not guaranteed to be equivalent values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of means: 0.4688591570581349\n",
      "Mean of all values: 0.4708584894709233\n"
     ]
    }
   ],
   "source": [
    "test = np.random.rand(10)\n",
    "test2 = np.random.rand(10)\n",
    "test3 = np.random.rand(20)\n",
    "\n",
    "# Mean of means\n",
    "print('Mean of means: %s' % np.mean([np.mean(test), np.mean(test2), np.mean(test3)]))\n",
    "\n",
    "# Mean of all values\n",
    "print('Mean of all values: %s' % (np.sum([np.sum(test),np.sum(test2),np.sum(test3)]) / 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null_dask(dd, cols):\n",
    "    '''\n",
    "    input: dask dataframe, numerical columns with null values\n",
    "    output: dask dataframe with null values imputed with mean and a new feature indicating that entry was null\n",
    "    '''\n",
    "    #dd.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        \n",
    "        # unfortunately, have to use some trickery here as the new column assignment is not a string\n",
    "        eval('dd.assign(%s=data.loc[:,\"%s\"].isnull().astype(np.uint8))' % (feat_is_null, col))\n",
    "        \n",
    "         # we're going to have to aggregate with a tuple to get both the total sum and number of rows\n",
    "        sum_len = dd.loc[:,col].map_partitions(lambda x : (np.nansum(x), len(x))).compute()\n",
    "        sum_len = reduce(lambda x, y : (x[0] + y[0], x[1] + y[1]), sum_len)\n",
    "        impute_val = sum_len[0] / sum_len[1]\n",
    "        dd = data.map_partitions(lambda df : df.loc[:,col].fillna(impute_val))\n",
    "    \n",
    "    return dd  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our label encoder takes all of the possible values within a feature and creates a numerical mapping.  However, since our data will be split randomly into parititons, we cannot be certain that each partition will have an identical set of values.  In this example, I split the data into 12 partitions, but for this function, I do not want 12 potential encodings.\n",
    "\n",
    "We'll change the function to, instead, aggregate all the potential encodings before creating a numerical mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_encode_train_dask(dd, col, unkStr):\n",
    "    '''\n",
    "    input: Dask Dataframe\n",
    "    output: Dictionary containing categorical-to-numerical mappings\n",
    "    note: Need to be careful to grab all potential mappings across all partitions\n",
    "    '''\n",
    "    tmp_ = dd.map_partitions(lambda df : set(df[col])).compute()\n",
    "    keys = reduce(lambda x, y : x | y, tmp_)\n",
    "    keys.add(unkStr)\n",
    "    values = np.arange(0, len(keys)+1)\n",
    "    \n",
    "    return dict(zip(keys,values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 100000\n",
      "192 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "29.3 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "62.6 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "9.62 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "460 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 1000000\n",
      "271 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "30.7 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "177 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "16.8 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "1.09 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 10000000\n",
      "1.32 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "82.8 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "1.02 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "202 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "6.64 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "results_dd = pd.DataFrame(columns = [\n",
    "                                     'dd_date',\n",
    "                                     'dd_impute',\n",
    "                                     'dd_encode',\n",
    "                                     'dd_total',\n",
    "                                     'nRows'\n",
    "                                    ])\n",
    "\n",
    "# parameters for generating data\n",
    "startdate = datetime.date(1900,1,1)\n",
    "\n",
    "for i,nIter in enumerate(10 ** np.arange(5,8)): \n",
    "    data_ = gen_data(nIter, startdate)\n",
    "    data = dd.from_pandas(data_, npartitions=12)\n",
    "    \n",
    "    print('Size: %s'% nIter)\n",
    "    # date features\n",
    "    tmp_ = %timeit -n 1 -r 1 -o  data.\\\n",
    "                                    map_partitions(lambda df : date_extractor(df,'date')).\\\n",
    "                                    compute()\n",
    "    out1_ = tmp_.average\n",
    "    \n",
    "    # impute nulls\n",
    "    tmp_ = %timeit -n 1 -r 1 -o  impute_null_dask(data, ['c2']).compute()\n",
    "    out2_ = tmp_.average\n",
    "    data = dd.from_pandas(data_, npartitions=12)\n",
    "    tmp_ = %timeit -n 1 -r 1 -o  data.\\\n",
    "                                    map_partitions(lambda df : impute_null_cat(df, ['c3'], 'UNK')).\\\n",
    "                                    compute()\n",
    "    out2_ += tmp_.average\n",
    "    \n",
    "    # encode cat features\n",
    "    tmp_ = %timeit -n 1 -r 1 -o cat_encode_train_dask(data, 'c3', 'UNK')\n",
    "    out3_ = tmp_.average\n",
    "    dict_ = cat_encode_train_dask(data.\\\n",
    "                                   map_partitions(lambda df : impute_null_cat(df, ['c3'], 'UNK')),\n",
    "                                  'c3',\n",
    "                                  'UNK')\n",
    "    data = dd.from_pandas(data_, npartitions=12)\n",
    "    tmp_ = %timeit -n 1 -r 1 -o data.\\\n",
    "                                    map_partitions(lambda df : date_extractor(df,'date')).\\\n",
    "                                    map_partitions(lambda df : impute_null(df, ['c2'])).\\\n",
    "                                    map_partitions(lambda df : impute_null_cat(df, ['c3'], 'UNK')).\\\n",
    "                                    map_partitions(lambda df: cat_encoder(df, 'c3', dict_, 'UNK')).\\\n",
    "                                    compute()\n",
    "    out3_ += tmp_.average\n",
    "    \n",
    "    row = {'dd_date':out1_,\n",
    "           'dd_impute':out2_,\n",
    "           'dd_encode':out3_-out2_-out1_,\n",
    "           'dd_total':out3_,\n",
    "           'nRows':nIter,\n",
    "          }\n",
    "    \n",
    "    results_dd.loc[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dd_date</th>\n",
       "      <th>dd_impute</th>\n",
       "      <th>dd_encode</th>\n",
       "      <th>dd_total</th>\n",
       "      <th>nRows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.191643</td>\n",
       "      <td>0.091936</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.469853</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.271246</td>\n",
       "      <td>0.207563</td>\n",
       "      <td>0.632007</td>\n",
       "      <td>1.110816</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.320026</td>\n",
       "      <td>1.105501</td>\n",
       "      <td>4.419274</td>\n",
       "      <td>6.844801</td>\n",
       "      <td>10000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dd_date  dd_impute  dd_encode  dd_total       nRows\n",
       "0  0.191643   0.091936   0.186275  0.469853    100000.0\n",
       "1  0.271246   0.207563   0.632007  1.110816   1000000.0\n",
       "2  1.320026   1.105501   4.419274  6.844801  10000000.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "For this section, we will need to rewrite much of the previous work to utilize Spark.  Additionally, we'll need to configure the various components of Spark when we initialize a Spark Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "conf = SparkConf().\\\n",
    "        setMaster('local[*]').\\\n",
    "        setAppName('parallel_preprocessing').\\\n",
    "        set('spark.driver.cores','1').\\\n",
    "        set('spark.num.executors','5').\\\n",
    "        set('spark.driver.memory', '4G').\\\n",
    "        set('spark.executor.memory', '5G').\\\n",
    "        set('spark.driver.maxResultSize', '5G')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqc = SQLContext.getOrCreate(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like Python, datetime objects in Spark can be split into their constituent components.  Unfortunately, not every piece behaves like its Python counterpart.  In our case, the default 'dayofweek' will be encoded as a string `'Sunday', 'Monday',...` when we want integers `0, 1,...`.  Also unfortunate, changing data types is not the easiest thing in the world for a Spark Data Frame, but we can use `select` and `cast` to take care of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_date_extractor(sdf, dateCol):\n",
    "    '''\n",
    "    input: Spark dataframe, column of datetime objects\n",
    "    output: Spark dataframe with new time-interval features appended\n",
    "    notes: Unfortunately, there's a lot of hard coding here for the individual features\n",
    "    ''' \n",
    "    str_to_int = {\n",
    "              'Sunday':'0', \n",
    "              'Monday':'1',\n",
    "              'Tuesday':'2',\n",
    "              'Wednesday':'3',\n",
    "              'Thursday':'4',\n",
    "              'Friday':'5',\n",
    "              'Saturday':'6',\n",
    "             }\n",
    "    \n",
    "    sdf = sdf.\\\n",
    "    withColumn('year', year(dateCol)).\\\n",
    "    withColumn('month', month(dateCol)).\\\n",
    "    withColumn('day', dayofmonth(dateCol)).\\\n",
    "    withColumn('week', weekofyear(dateCol)).\\\n",
    "    withColumn('dayofweek', date_format(dateCol,'EEEE')).\\\n",
    "    na.replace(str_to_int, 1, 'dayofweek').\\\n",
    "    selectExpr('c1', 'c2', 'c3', 'year', 'month', 'day', 'week', 'cast(dayofweek as int) dayofweek')\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using the mean value to impute missing continuous values, we'll want to collect all possible values across the partition first as this simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_impute_null(sdf, cols):\n",
    "    '''\n",
    "    input: Spark Dataframe, numerical columns missing values\n",
    "    output: Spark Dataframe with missing values filled with mean\n",
    "    '''\n",
    "    for col in cols:\n",
    "        value = sdf.\\\n",
    "                filter(~isnan(col)).\\\n",
    "                select(avg(col)).\\\n",
    "                head()[0]\n",
    "        \n",
    "        sdf = sdf.na.fill({col:value})\n",
    "    \n",
    "    return sdf\n",
    "\n",
    "\n",
    "def spark_impute_null_cat(sdf, cols, unkStr):\n",
    "    '''\n",
    "    input: Spark Dataframe, categorical columns missing values, new value for missing\n",
    "    output: Spark Dataframe with missing value replaced with unkStr\n",
    "    '''\n",
    "    for col in cols:\n",
    "        sdf = sdf.withColumn(col, regexp_replace(col, 'NaN', 'UNK'))\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like in Dask, we'll need to be sure to gather all possible values across the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_cat_encode_train(sdf, col, unkStr):\n",
    "    '''\n",
    "    input: spark data frame, col to encode, unkStr to append for future unknowns\n",
    "    output: dictionary containing previous values and numeric encodings\n",
    "    '''\n",
    "    keys = sdf.select(col).rdd.reduce(lambda x, y : set(x) | set(y))\n",
    "    keys.add(unkStr)\n",
    "    values = np.arange(0, len(keys)+1).astype(str)\n",
    "    \n",
    "    return dict(zip(keys,values))\n",
    "    \n",
    "def spark_cat_encoder(sdf, col, dict_enc, unkStr):\n",
    "    '''\n",
    "    input: spark dataframe, col to encode, dictionary of encodings, unkStr for unknowns\n",
    "    output: spark dataframe with string column encoded as integer\n",
    "    '''\n",
    "    # need to recode unknown values to unkStr\n",
    "    cast_unk = udf(lambda x : unkStr if x not in dict_enc else x, StringType())\n",
    "    sdf = sdf.withColumn(col, cast_unk(col))\n",
    "    \n",
    "    # now make the replacement according to dict_enc\n",
    "    sdf = sdf.na.replace(dict_enc, 1, col)\n",
    "    # need this monstrosity to select all cols with col recast as int\n",
    "    sdf = eval('sdf.selectExpr(\"'+\\\n",
    "               '\",\"'.join(['cast(%s as int) %s' % (i,i) if i == col else i for i in sdf.columns])+\\\n",
    "               '\")'\n",
    "              )\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sp = pd.DataFrame(columns = [\n",
    "                                     'sp_date',\n",
    "                                     'sp_impute',\n",
    "                                     'sp_encode',\n",
    "                                     'sp_total',\n",
    "                                     'nRows'\n",
    "                                    ])\n",
    "\n",
    "# parameters for generating data\n",
    "startdate = datetime.date(1900,1,1)\n",
    "\n",
    "for i,nIter in enumerate(row_range): \n",
    "    data = gen_data(nIter, startdate)\n",
    "    data_schema = StructType([\n",
    "                              StructField('c1',DoubleType(),True),\n",
    "                              StructField('c2',DoubleType(),True),\n",
    "                              StructField('c3',StringType(),True),\n",
    "                              StructField('date',DateType(),True)\n",
    "                             ])\n",
    "\n",
    "    data = sqc.createDataFrame(data, data_schema)\n",
    "    \n",
    "    print('Size: %s'% nIter)\n",
    "    # date features\n",
    "    tmp_ = %timeit -n 1 -r 1 -o  \n",
    "    out1_ = tmp_.average\n",
    "    \n",
    "    # impute nulls\n",
    "    tmp_ = %timeit -n 1 -r 1 -o  \n",
    "    out2_ = tmp_.average\n",
    "\n",
    "    tmp_ = %timeit -n 1 -r 1 -o  \n",
    "    out2_ += tmp_.average\n",
    "    \n",
    "    # encode cat features\n",
    "    tmp_ = %timeit -n 1 -r 1 -o \n",
    "    out3_ = tmp_.average\n",
    "    dict_ = \n",
    "    tmp_ = %timeit -n 1 -r 1 -o \n",
    "    out3_ += tmp_.average\n",
    "    \n",
    "    row = {'sp_date':out1_,\n",
    "           'sp_impute':out2_,\n",
    "           'sp_encode':out3_-out2_-out1_,\n",
    "           'sp_total':out3_,\n",
    "           'nRows':nIter,\n",
    "          }\n",
    "    \n",
    "    results_sp.loc[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot((results_dates.nRows), results_dates.pandas, label='Python')\n",
    "ax.set_xlabel(\"Rows\")\n",
    "ax.set_ylabel(\"Time(Seconds)\")\n",
    "ax.set_xscale('log')\n",
    "ax.axis([1,100000,0,.05])\n",
    "ax.legend()\n",
    "#fig.savefig('./python_numba_only.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = datetime.date(1900,1,1)\n",
    "data = gen_data(100_000,startdate)\n",
    "\n",
    "data_schema = StructType([\n",
    "                          StructField('c1',DoubleType(),True),\n",
    "                          StructField('c2',DoubleType(),True),\n",
    "                          StructField('c3',StringType(),True),\n",
    "                          StructField('date',DateType(),True)\n",
    "                         ])\n",
    "\n",
    "data_spark = sqc.createDataFrame(data,data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
