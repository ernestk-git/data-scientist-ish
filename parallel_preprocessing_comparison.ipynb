{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from numba import jit\n",
    "from multiprocessing import cpu_count\n",
    "from dask.multiprocessing import get\n",
    "from dask import set_options\n",
    "from functools import reduce\n",
    "\n",
    "nCores = cpu_count()\n",
    "\n",
    "#set_options(get=dask.get)\n",
    "\n",
    "# parameters for generating data\n",
    "row_range = 10 ** np.arange(4,8)\n",
    "startdate = datetime.date(1900,1,1)\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Data\n",
    "\n",
    "The computer I primarily use has 32GB of RAM (DDR4-3200,CAS14).  Additionally, I created a 64GB swap file partition on a Samsung 960 Evo M.2 Flash Drive (if anyone has any experience using Intel Optane drive for this, let me know about your experiences).  I wanted to create a dataframe that exceeded 32GB in memory to test the efficacy of Pandas vs Dask vs Spark.  The following parameters should accomplish that with 100 million rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"data\" contains a random date between 1900 and 2000, a random float between 0 and 1, a random int between 0 and 3333, and a \"categorical\" string of [a-z].\n",
    "\n",
    "The random int column, c2, contains NaN values, 10% of the total.\n",
    "\n",
    "The categorical column, c3, also contains NaN values, 10% of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(nRows, startdate):\n",
    "    # generate random data\n",
    "    c1 = np.random.rand(nRows)\n",
    "    c2 = np.random.randint(0,3333,nRows)\n",
    "    c3 = np.random.choice(['a','b','c','d','e',\n",
    "               'f','g','h','i','j',\n",
    "               'k','l','m','n','o',\n",
    "               'p','q','r','s','t',\n",
    "               'u','v','w','x','y','z'],size=nRows)\n",
    "    date = [startdate + datetime.timedelta(int(i)) for i in np.random.randint(1,36524,size=nRows)]\n",
    "    data = pd.DataFrame({'date':date,\n",
    "                         'c1':c1,\n",
    "                         'c2':c2,\n",
    "                         'c3':c3\n",
    "                        })\n",
    "    data.date = pd.to_datetime(data.date)\n",
    "\n",
    "    # picking random null values\n",
    "    data.loc[data.sample(frac=.1).index,'c2'] = np.nan\n",
    "    data.loc[data.sample(frac=.1).index,'c3'] = np.nan\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python, Pandas, Single-Processor\n",
    "\n",
    "The following function extracts the various parts of time from a datetime column and returns the original dataframe with the new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_extractor(df, dateCol='date', interval=['year','month','week','day','dayofweek']):\n",
    "    '''\n",
    "    input: dataframe, column of datetime objects, desired list of time-interval features\n",
    "    output: dataframe with new time-interval features appended\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for i in interval:\n",
    "        df.loc[:, i] = eval('df.%s.dt.%s'% (dateCol, i))\n",
    "    df.drop(dateCol, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function fills our missing values and creates an addition feature: `feat_isnull`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null(df, cols):\n",
    "    '''\n",
    "    input: dataframe, numerical columns with null values\n",
    "    output: dataframe with null values imputed with mean and a new feature indicating that entry was null\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        df.loc[:, feat_is_null] = np.int8(df[col].isnull())\n",
    "        \n",
    "         # imputing median\n",
    "        impute_value = np.nanmean(df[col])\n",
    "        df.loc[:, col].fillna(impute_value, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical features, we'll treat them slightly differently by passing a string we wish to use as a null placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null_cat(df, cols, naStr):\n",
    "    '''\n",
    "    input: dataframe, categorical column with null values, string to signify missing value\n",
    "    output: dataframe with null values imputed with 'UNK' and a new feature indicating entry was null\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        df.loc[:, feat_is_null] = np.int8(df[col].isnull())\n",
    "        \n",
    "        # imputing missing code\n",
    "        df.loc[:, col].fillna(naStr, inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions deal with encoding categorical features.  We will not be using the sklearn encoder as it has issues dealing with previously unseen values.  For example, let's say you have a feature with 6 levels: `{A, B, C, D, E, F}`, however, `F` is relatively rare.  After creating a validation set, it turns out that `F` is not seen in the `x_train` but is in `x_valid`.  \n",
    "\n",
    "Sklearn's encoder may not be able to \"train\" on `x_train` and properly convert `x_valid` due to the varying number of values.  In the functions below, instead, we pass `unkStr` to recode a previously unseen value.  For this example, we will encode these as we did missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_encode_train(df, col, unkStr):\n",
    "    '''\n",
    "    input: dataframe, name of single categorical column\n",
    "    output: dictionary representing previous levels and new numerical encodings\n",
    "    '''\n",
    "    keys = set(df[col])\n",
    "    keys.add(unkStr)\n",
    "    values = np.arange(0, len(keys)+1)\n",
    "    \n",
    "    return dict(zip(keys,values))\n",
    "\n",
    "\n",
    "def cat_encoder(df, col, dict_enc, unkStr):\n",
    "    '''\n",
    "    input: dataframe, name of single categorical column, dictionary of encodings, string to use as unknown\n",
    "    output: dataframe with categorical values encoded\n",
    "    note: you probably want to match the unknown str with the string used as null in impute_null_cat\n",
    "    '''\n",
    "    df.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    # need to replace unknown values with unkStr\n",
    "    df.loc[~df[col].isin(set(dict_enc.keys())), col] = unkStr\n",
    "\n",
    "    df.loc[:,col] = df.loc[:,col].map(lambda x : dict_enc[x])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask, Multi-Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next section, we'll be using map_partitions to apply the previous written functions across data sets to parallelize their operations.  While we could re-write these functions, this is the quickest way to get the benefit of multi-core parallel operation.  However, we're going to have to rewrite our `impute_null()` and `cat_encoder()` functions due to the distributed nature of Dask.\n",
    "\n",
    "For imputing the mean on a continuous variable, we'll need to collect all possible values and calculate the mean (instead of taking the mean across partitions, and finding the mean of that).  This next block illustrate that these are not guaranteed to be equivalent values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of means: 0.4850058991578026\n",
      "Mean of all values: 0.5147681510492076\n"
     ]
    }
   ],
   "source": [
    "test = np.random.rand(10)\n",
    "test2 = np.random.rand(10)\n",
    "test3 = np.random.rand(20)\n",
    "\n",
    "# Mean of means\n",
    "print('Mean of means: %s' % np.mean([np.mean(test), np.mean(test2), np.mean(test3)]))\n",
    "\n",
    "# Mean of all values\n",
    "print('Mean of all values: %s' % (np.sum([np.sum(test),np.sum(test2),np.sum(test3)]) / 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_null_dask(dd, cols):\n",
    "    '''\n",
    "    input: dask dataframe, numerical columns with null values\n",
    "    output: dask dataframe with null values imputed with mean and a new feature indicating that entry was null\n",
    "    '''\n",
    "    #dd.is_copy = False # we're not dealing with copies, this avoids the warning\n",
    "    for col in cols:\n",
    "        # creating the new feature which indicates isnull\n",
    "        feat_is_null = col + '_isnull'\n",
    "        \n",
    "        # unfortunately, have to use some trickery here as the new column assignment is not a string\n",
    "        eval('dd.assign(%s=data.loc[:,\"%s\"].isnull().astype(np.uint8))' % (feat_is_null, col))\n",
    "        \n",
    "         # we're going to have to aggregate with a tuple to get both the total sum and number of rows\n",
    "        sum_len = dd.loc[:,col].map_partitions(lambda x : (np.nansum(x), len(x))).compute()\n",
    "        sum_len = reduce(lambda x, y : (x[0] + y[0], x[1] + y[1]), sum_len)\n",
    "        impute_val = sum_len[0] / sum_len[1]\n",
    "        dd = data.map_partitions(lambda df : df.loc[:,col].fillna(impute_val))\n",
    "    \n",
    "    return dd  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our label encoder takes all of the possible values within a feature and creates a numerical mapping.  However, since our data will be split randomly into parititons, we cannot be certain that each partition will have an identical set of values.  In this example, I split the data into 12 partitions, but for this function, I do not want 12 potential encodings.\n",
    "\n",
    "We'll change the function to, instead, aggregate all the potential encodings before creating a numerical mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_encode_train_dask(dd, col, unkStr):\n",
    "    '''\n",
    "    input: Dask Dataframe\n",
    "    output: Dictionary containing categorical-to-numerical mappings\n",
    "    note: Need to be careful to grab all potential mappings across all partitions\n",
    "    '''\n",
    "    tmp_ = dd.map_partitions(lambda df : set(df[col])).compute()\n",
    "    keys = reduce(lambda x, y : x | y, tmp_)\n",
    "    keys.add(unkStr)\n",
    "    values = np.arange(0, len(keys)+1)\n",
    "    \n",
    "    return dict(zip(keys,values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "For this section, we will need to rewrite much of the previous work to utilize Spark.  Additionally, we'll need to configure the various components of Spark when we initialize a Spark Context.\n",
    "\n",
    "Much like Python, datetime objects in Spark can be split into their constituent components.  Unfortunately, not every piece behaves like its Python counterpart.  In our case, the default 'dayofweek' will be encoded as a string `'Sunday', 'Monday',...` when we want integers `0, 1,...`.  Also unfortunate, changing data types is not the easiest thing in the world for a Spark Data Frame, but we can use `select` and `cast` to take care of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_date_extractor(sdf, dateCol):\n",
    "    '''\n",
    "    input: Spark dataframe, column of datetime objects\n",
    "    output: Spark dataframe with new time-interval features appended\n",
    "    notes: Unfortunately, there's a lot of hard coding here for the individual features\n",
    "    ''' \n",
    "    str_to_int = {\n",
    "              'Sunday':'0', \n",
    "              'Monday':'1',\n",
    "              'Tuesday':'2',\n",
    "              'Wednesday':'3',\n",
    "              'Thursday':'4',\n",
    "              'Friday':'5',\n",
    "              'Saturday':'6',\n",
    "             }\n",
    "    \n",
    "    sdf = sdf.\\\n",
    "    withColumn('year', year(dateCol)).\\\n",
    "    withColumn('month', month(dateCol)).\\\n",
    "    withColumn('day', dayofmonth(dateCol)).\\\n",
    "    withColumn('week', weekofyear(dateCol)).\\\n",
    "    withColumn('dayofweek', date_format(dateCol,'EEEE')).\\\n",
    "    na.replace(str_to_int, 1, 'dayofweek').\\\n",
    "    selectExpr('c1', 'c2', 'c3', 'year', 'month', 'day', 'week', 'cast(dayofweek as int) dayofweek')\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using the mean value to impute missing continuous values, we'll want to collect all possible values across the partitions as we did with Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_impute_null(sdf, cols):\n",
    "    '''\n",
    "    input: Spark Dataframe, numerical columns missing values\n",
    "    output: Spark Dataframe with missing values filled with mean\n",
    "    '''\n",
    "    for col in cols:\n",
    "        value = sdf.\\\n",
    "                filter(~isnan(col)).\\\n",
    "                select(avg(col)).\\\n",
    "                head()[0]\n",
    "        \n",
    "        sdf = sdf.na.fill({col:value})\n",
    "    \n",
    "    return sdf\n",
    "\n",
    "\n",
    "def spark_impute_null_cat(sdf, cols, unkStr):\n",
    "    '''\n",
    "    input: Spark Dataframe, categorical columns missing values, new value for missing\n",
    "    output: Spark Dataframe with missing value replaced with unkStr\n",
    "    '''\n",
    "    for col in cols:\n",
    "        sdf = sdf.withColumn(col, regexp_replace(col, 'NaN', 'UNK'))\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like in Dask, we'll need to be sure to gather all possible values across the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_cat_encode_train(sdf, col, unkStr):\n",
    "    '''\n",
    "    input: spark data frame, col to encode, unkStr to append for future unknowns\n",
    "    output: dictionary containing previous values and numeric encodings\n",
    "    '''\n",
    "    keys = sdf.select(col).rdd.reduce(lambda x, y : set(x) | set(y))\n",
    "    keys.add(unkStr)\n",
    "    values = np.arange(0, len(keys)+1).astype(str)\n",
    "    \n",
    "    return dict(zip(keys,values))\n",
    "    \n",
    "def spark_cat_encoder(sdf, col, dict_enc, unkStr):\n",
    "    '''\n",
    "    input: spark dataframe, col to encode, dictionary of encodings, unkStr for unknowns\n",
    "    output: spark dataframe with string column encoded as integer\n",
    "    '''\n",
    "    # need to recode unknown values to unkStr\n",
    "    cast_unk = udf(lambda x : unkStr if x not in dict_enc else x, StringType())\n",
    "    sdf = sdf.withColumn(col, cast_unk(col))\n",
    "    \n",
    "    # now make the replacement according to dict_enc\n",
    "    sdf = sdf.na.replace(dict_enc, 1, col)\n",
    "    # need this monstrosity to select all cols with col recast as int\n",
    "    sdf = eval('sdf.selectExpr(\"'+\\\n",
    "               '\",\"'.join(['cast(%s as int) %s' % (i,i) if i == col else i for i in sdf.columns])+\\\n",
    "               '\")'\n",
    "              )\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 10000\n",
      "9.08 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "1.65 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "2.12 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "165 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "6.63 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 100000\n",
      "39 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "3.1 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "8.78 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "1.03 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "36.6 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 1000000\n",
      "340 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "17.6 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "73.4 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "10.8 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "334 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 10000000\n",
      "4.63 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "305 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "864 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "122 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "3.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "results_py = pd.DataFrame(columns = [\n",
    "                                     'date',\n",
    "                                     'impute',\n",
    "                                     'encode',\n",
    "                                     'total',\n",
    "                                     'nRows',\n",
    "                                     'memory'\n",
    "                                    ])\n",
    "\n",
    "for i,nIter in enumerate(row_range): \n",
    "    data = gen_data(nIter, startdate)\n",
    "    out1_, out2_, out3_ = 0, 0, 0\n",
    "    # date features\n",
    "    print('Size: %s'% nIter)\n",
    "    tmp_ = %timeit -n 1 -r 1 -o date_extractor(data, 'date')\n",
    "    out1_ = tmp_.average\n",
    "    \n",
    "    # imputing nulls\n",
    "    tmp_ = %timeit -n 1 -r 1 -o impute_null(data, ['c2'])\n",
    "    out2_ = tmp_.average\n",
    "    tmp_ = %timeit -n 1 -r 1 -o impute_null_cat(data, ['c3'], 'UNK')\n",
    "    out2_ += tmp_.average\n",
    "    \n",
    "    # encode cat features\n",
    "    tmp_ = %timeit -n 1 -r 1 -o cat_encode_train(data, 'c3', 'UNK')\n",
    "    out3_ = tmp_.average\n",
    "    dict_ = cat_encode_train(data, 'c3', 'UNK')\n",
    "    tmp_ = %timeit -n 1 -r 1 -o cat_encoder(data, 'c3', dict_, 'UNK')\n",
    "    out3_ += tmp_.average\n",
    "    \n",
    "    row = {'date':out1_,\n",
    "           'impute':out2_,\n",
    "           'encode':out3_,\n",
    "           'total':out1_ + out2_ + out3_,\n",
    "           'nRows':nIter,\n",
    "           'memory':data.memory_usage().sum() / 1e6\n",
    "          }\n",
    "    \n",
    "    results_py.loc[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 10000\n",
      "81 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "15.1 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "22.7 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "17.7 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "160 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 100000\n",
      "110 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "16.2 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "29.8 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "24.6 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "225 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 1000000\n",
      "429 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "22 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "103 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "95 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "937 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 10000000\n",
      "3.58 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "101 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "909 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "814 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "7.96 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "results_dd = pd.DataFrame(columns = [\n",
    "                                     'date',\n",
    "                                     'impute',\n",
    "                                     'encode',\n",
    "                                     'total',\n",
    "                                     'nRows'\n",
    "                                    ])\n",
    "\n",
    "for i,nIter in enumerate(row_range): \n",
    "    data_ = gen_data(nIter, startdate)\n",
    "    data = dd.from_pandas(data_, npartitions=nCores)\n",
    "    out1_, out2_, out3_ = 0, 0, 0\n",
    "    print('Size: %s'% nIter)\n",
    "    # date features\n",
    "    tmp_ = %timeit -n 1 -r 1 -o  data.\\\n",
    "                                    map_partitions(lambda df : date_extractor(df,'date')).\\\n",
    "                                    compute()\n",
    "    time_date = tmp_.average\n",
    "    \n",
    "    # impute nulls\n",
    "    tmp_ = %timeit -n 1 -r 1 -o  impute_null_dask(data, ['c2']).compute()\n",
    "    time_null = tmp_.average\n",
    "    \n",
    "    data = dd.from_pandas(data_, npartitions=nCores)\n",
    "    tmp_ = %timeit -n 1 -r 1 -o  data.\\\n",
    "                                    map_partitions(lambda df : impute_null_cat(df, ['c3'], 'UNK')).\\\n",
    "                                    compute()\n",
    "    time_null_cat = tmp_.average\n",
    "    \n",
    "    # encode cat features\n",
    "    data = dd.from_pandas(data_, npartitions=nCores)\n",
    "    data = data.map_partitions(lambda df : impute_null_cat(df, ['c3'], 'UNK'))\n",
    "    tmp_ = %timeit -n 1 -r 1 -o cat_encode_train_dask(data, 'c3','UNK')\n",
    "    time_dict = tmp_.average\n",
    "    \n",
    "    dict_ = cat_encode_train_dask(data.\\\n",
    "                                   map_partitions(lambda df : impute_null_cat(df, ['c3'], 'UNK')),\n",
    "                                  'c3',\n",
    "                                  'UNK')\n",
    "    data = dd.from_pandas(data_, npartitions=nCores)\n",
    "    tmp_ = %timeit -n 1 -r 1 -o data.\\\n",
    "                                    map_partitions(lambda df : date_extractor(df,'date')).\\\n",
    "                                    map_partitions(lambda df : impute_null(df, ['c2'])).\\\n",
    "                                    map_partitions(lambda df : impute_null_cat(df, ['c3'], 'UNK')).\\\n",
    "                                    map_partitions(lambda df: cat_encoder(df, 'c3', dict_, 'UNK')).\\\n",
    "                                    compute()\n",
    "    time_encode = tmp_.average\n",
    "    \n",
    "    row = {'date':time_date,\n",
    "           'impute':time_null + time_null_cat,\n",
    "           'encode':time_encode + time_dict - time_null - time_null_cat - time_null_cat - time_date,\n",
    "           'total':time_encode + time_dict - time_null_cat,\n",
    "           'nRows':nIter,\n",
    "          }\n",
    "    \n",
    "    results_dd.loc[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imarobit/spark/python/pyspark/sql/dataframe.py:1431: UserWarning:\n",
      "\n",
      "to_replace is a dict and value is not None. value will be ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.84 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "853 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "261 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "230 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imarobit/spark/python/pyspark/sql/dataframe.py:1431: UserWarning:\n",
      "\n",
      "to_replace is a dict and value is not None. value will be ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imarobit/spark/python/pyspark/sql/dataframe.py:1431: UserWarning:\n",
      "\n",
      "to_replace is a dict and value is not None. value will be ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "801 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "672 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "189 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imarobit/spark/python/pyspark/sql/dataframe.py:1431: UserWarning:\n",
      "\n",
      "to_replace is a dict and value is not None. value will be ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imarobit/spark/python/pyspark/sql/dataframe.py:1431: UserWarning:\n",
      "\n",
      "to_replace is a dict and value is not None. value will be ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.29 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "5.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "5.28 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "1.08 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imarobit/spark/python/pyspark/sql/dataframe.py:1431: UserWarning:\n",
      "\n",
      "to_replace is a dict and value is not None. value will be ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.17 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Size: 10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imarobit/spark/python/pyspark/sql/dataframe.py:1431: UserWarning:\n",
      "\n",
      "to_replace is a dict and value is not None. value will be ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "57.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "55.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "11.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imarobit/spark/python/pyspark/sql/dataframe.py:1431: UserWarning:\n",
      "\n",
      "to_replace is a dict and value is not None. value will be ignored.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 15s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "conf = SparkConf().\\\n",
    "        setMaster('local[*]').\\\n",
    "        setAppName('parallel_preprocessing').\\\n",
    "        set('spark.shuffle.file.buffer', '128k').\\\n",
    "        set('spark.executor.memory', '5g').\\\n",
    "        set('spark.driver.memory', '4g').\\\n",
    "        set('spark.executor.instances', '5')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqc = SQLContext.getOrCreate(sc)\n",
    "sqc.sql(\"set spark.sql.shuffle.partitions=12\")\n",
    "\n",
    "results_sp = pd.DataFrame(columns = [\n",
    "                                     'date',\n",
    "                                     'impute',\n",
    "                                     'encode',\n",
    "                                     'total',\n",
    "                                     'nRows'\n",
    "                                    ])\n",
    "\n",
    "data_schema = StructType([\n",
    "                          StructField('c1',DoubleType(),True),\n",
    "                          StructField('c2',DoubleType(),True),\n",
    "                          StructField('c3',StringType(),True),\n",
    "                          StructField('date',DateType(),True)\n",
    "                         ])\n",
    "\n",
    "for i,nIter in enumerate(row_range): \n",
    "    data = gen_data(nIter, startdate)\n",
    "    data = sqc.createDataFrame(data,data_schema)\n",
    "    out1_, out2_, out3_ = 0, 0, 0\n",
    "    print('Size: %s'% nIter)\n",
    "    # date features\n",
    "    tmp_ = %timeit -n 1 -r 1 -o spark_date_extractor(data, 'date').collect()\n",
    "    out1_ = tmp_.average\n",
    "    \n",
    "    # impute nulls\n",
    "    tmp_ = %timeit -n 1 -r 1 -o spark_impute_null(data, ['c2']).collect()\n",
    "    out2_ = tmp_.average\n",
    "    tmp_ = %timeit -n 1 -r 1 -o spark_impute_null_cat(data, ['c3'], 'UNK').collect()\n",
    "    out2_ += tmp_.average\n",
    "    \n",
    "    # encode cat features\n",
    "    tmp_ = %timeit -n 1 -r 1 -o spark_cat_encode_train(data, 'c3', 'UNK')\n",
    "    out3_ = tmp_.average\n",
    "    dict_ = spark_cat_encode_train(data, 'c3', 'UNK')\n",
    "    tmp_ = %timeit -n 1 -r 1 -o spark_cat_encoder(data, 'c3', dict_, 'UNK').collect()\n",
    "    out3_ += tmp_.average\n",
    "    \n",
    "    row = {'date':out1_,\n",
    "           'impute':out2_,\n",
    "           'encode':out3_,\n",
    "           'total':out3_+out2_+out1_,\n",
    "           'nRows':nIter,\n",
    "          }\n",
    "    \n",
    "    results_sp.loc[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>impute</th>\n",
       "      <th>encode</th>\n",
       "      <th>total</th>\n",
       "      <th>nRows</th>\n",
       "      <th>memory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009082</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.019648</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.66008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038953</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>0.037624</td>\n",
       "      <td>0.088456</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>6.60008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.340248</td>\n",
       "      <td>0.090996</td>\n",
       "      <td>0.344405</td>\n",
       "      <td>0.775650</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>66.00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.633679</td>\n",
       "      <td>1.168647</td>\n",
       "      <td>3.824533</td>\n",
       "      <td>9.626858</td>\n",
       "      <td>10000000.0</td>\n",
       "      <td>660.00008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date    impute    encode     total       nRows     memory\n",
       "0  0.009082  0.003766  0.006799  0.019648     10000.0    0.66008\n",
       "1  0.038953  0.011879  0.037624  0.088456    100000.0    6.60008\n",
       "2  0.340248  0.090996  0.344405  0.775650   1000000.0   66.00008\n",
       "3  4.633679  1.168647  3.824533  9.626858  10000000.0  660.00008"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>impute</th>\n",
       "      <th>encode</th>\n",
       "      <th>total</th>\n",
       "      <th>nRows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.080962</td>\n",
       "      <td>0.037824</td>\n",
       "      <td>0.035990</td>\n",
       "      <td>0.154775</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.109699</td>\n",
       "      <td>0.045962</td>\n",
       "      <td>0.064066</td>\n",
       "      <td>0.219727</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.429397</td>\n",
       "      <td>0.124537</td>\n",
       "      <td>0.375819</td>\n",
       "      <td>0.929753</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.584429</td>\n",
       "      <td>1.010302</td>\n",
       "      <td>3.269169</td>\n",
       "      <td>7.863899</td>\n",
       "      <td>10000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date    impute    encode     total       nRows\n",
       "0  0.080962  0.037824  0.035990  0.154775     10000.0\n",
       "1  0.109699  0.045962  0.064066  0.219727    100000.0\n",
       "2  0.429397  0.124537  0.375819  0.929753   1000000.0\n",
       "3  3.584429  1.010302  3.269169  7.863899  10000000.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>impute</th>\n",
       "      <th>encode</th>\n",
       "      <th>total</th>\n",
       "      <th>nRows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.837032</td>\n",
       "      <td>1.113834</td>\n",
       "      <td>0.719530</td>\n",
       "      <td>3.670396</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.750208</td>\n",
       "      <td>1.472716</td>\n",
       "      <td>1.297432</td>\n",
       "      <td>3.520355</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.292042</td>\n",
       "      <td>10.775711</td>\n",
       "      <td>8.243677</td>\n",
       "      <td>23.311429</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.573891</td>\n",
       "      <td>112.835483</td>\n",
       "      <td>87.218848</td>\n",
       "      <td>245.628223</td>\n",
       "      <td>10000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      impute     encode       total       nRows\n",
       "0   1.837032    1.113834   0.719530    3.670396     10000.0\n",
       "1   0.750208    1.472716   1.297432    3.520355    100000.0\n",
       "2   4.292042   10.775711   8.243677   23.311429   1000000.0\n",
       "3  45.573891  112.835483  87.218848  245.628223  10000000.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_py.to_feather('py_results')\n",
    "results_dd.to_feather('dd_results')\n",
    "results_sp.to_feather('sp_results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
